{
  "hash": "df918f97065b8121095cdf2218605967",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: '**Modeling**'\n---\n\n\n***\n## Notebook overview\n\nWe now arrive at the modeling step, using logistic regression for this classification task.\n\nAll preprocessing is done in a pipeline (defined in the previous notebook and custom class in /src) for consistency and readability. \n\nA 60/20/20 train/validation/test split enables hyperparameter and threshold tuning while preserving a holdout test set.\n\nLogistic regression was chosen for its interpretability. To give the model the best chance at predicting the minority class accurately, the training set is SMOTE-resampled. \n\nFinal evaluation uses the threshold that yields the best F1 score (0.79) to assess generalization.\n\n**Validation Set (Threshold = 0.79):**\n- ROC AUC: `0.827` | Accuracy: `0.874`\n- Precision: `0.632` | Recall: `0.511` | F1 Score: `0.565`\n\n**Test Set (Threshold = 0.79):**\n- ROC AUC: `0.808` | Accuracy: `0.861`\n- Precision: `0.571` | Recall: `0.511` | F1 Score: `0.539`\n\nOverall, results are consistent across sets, indicating strong generalization. The threshold favors confident positive predictions (given the high precision coupled with the high threshold), making the model well-suited for risk-sensitive HR decisions. Coefficient interpretation follows in the next notebook.\n\n## Notebook outline\n1. [Load dataset and preprocessing pipeline](#1-load-dataset-and-preprocessing-pipeline)  \n2. [Cross-validation evaluation](#2-cross-validation-evaluation)  \n3. [Threshold tuning](#3-threshold-tuning)  \n4. [Generalizability](#4-generalizability)  \n5. [Summary and exports](#summary-and-exports)\n\n\n\n\n# 1. Load dataset and preprocessing pipeline\n***\n\nThe preprocessing pipeline is fit only on the training set, then applied consistently across all splits. SMOTE is applied to the training set to balance the classes before modeling.\n\nAt this stage, all inputs are clean, numeric, schema-aligned, and ready for model fitting â€” with no leakage between train, validation, and test data.\n\n\n\n::: {#d3c6c680 .cell quarto-private-1='{\"key\":\"jupyter\",\"value\":{\"source_hidden\":true}}' execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of data_01.csv: (1470, 31)\nPreprocessing pipeline is compatible with current data.\nTrain shape: (882, 75), Val shape: (294, 75), Test shape: (294, 75)\nTrue\n```\n:::\n:::\n\n\n# 2. Cross-validation evaluation\n***\n\nWith the training data balanced using SMOTE, we tune our logistic regression model using GridSearch cross-validation (5-fold). \n\nGrid search selects a configuration that favors sparse coefficients and balanced class weights, achieving a cross-validated AUC of `0.896`.\n\nWe then evaluate this model on the validation set:\n- ROC AUC: `0.827`\n- The ROC curve shows strong separation of classes.\n- The confusion matrix shows solid precision but room to improve recall.\n\n::: {#788dfe2f .cell quarto-private-1='{\"key\":\"jupyter\",\"value\":{\"source_hidden\":true}}' execution_count=4}\n\n::: {.cell-output .cell-output-stdout}\n```\nFitting 5 folds for each of 16 candidates, totalling 80 fits\nValidation AUC: 0.8269\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](03_modeling_files/figure-html/cell-5-output-2.png){width=565 height=468}\n:::\n\n::: {.cell-output .cell-output-display}\n![](03_modeling_files/figure-html/cell-5-output-3.png){width=543 height=468}\n:::\n:::\n\n\n# 3. Threshold tuning\n***\n\nTo maximize results, we test thresholds from 0.01 to 0.99 to find the point that best balances precision and recall. \n\nThe optimal threshold based on F1 score is `0.79`. At this threshold: \n- F1 Score: `0.565`\n- Precision: `0.632`\n- Recall: `0.511`\n- Accuracy: `0.874`\n- AUC: `0.827`\n\nThis threshold greatly reduces false positives compared to the default (0.5), with a small trade-off in recall. \n\n::: {#7f089a48 .cell quarto-private-1='{\"key\":\"jupyter\",\"value\":{\"source_hidden\":true}}' execution_count=5}\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Threshold (by F1): 0.79\nF1 Score at Best Threshold: 0.565\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](03_modeling_files/figure-html/cell-6-output-2.png){width=757 height=372}\n:::\n\n::: {.cell-output .cell-output-display}\n![](03_modeling_files/figure-html/cell-6-output-3.png){width=504 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](03_modeling_files/figure-html/cell-6-output-4.png){width=504 height=449}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nEvaluation Metrics (Best Threshold):\nPrecision:  0.632\nRecall:     0.511\nF1 Score:   0.565\nAccuracy:   0.874\nROC AUC:    0.827\n```\n:::\n:::\n\n\n# 4. Generalizability\n***\n\nThe model has been tuned on SMOTE-resampled training data, and the optimal threshold has been found. Now we evaluate the model's generalization capability using the holdout test set. \n\n**Test Set Performance (Threshold = 0.79):**\n- Accuracy: `0.861`\n- Precision: `0.571`\n- Recall: `0.511`\n- F1 Score: `0.539`\n- ROC AUC: `0.808`\n\n**Result summary**: The metrics are strong but indicate room for improvement. The high accuracy score of 0.861 should be taken with a grain of salt - the target is highly imbalanced, so predicting No is correct more often than not. Precision and recall scores are lower than ideal - about half of the positive predictions are correct (precision), and half of the actual positives were correctly identified (recall) - but the model balances between false positives and false negatives reasonably well given the imbalanced target, as illustrated by the F1 score of 0.565. The AUC is strong, however, at 0.808, indicating that the model usually assigns higher probabilites to \"Yes\" values, which is the desired behavior. More complex models and perhaps more data to enrich this set could yield better results.\n\n::: {#fab2e739 .cell quarto-private-1='{\"key\":\"jupyter\",\"value\":{\"source_hidden\":true}}' execution_count=6}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nEvaluation on Final Test Set (Threshold = 0.79)\nAccuracy:  0.861\nPrecision: 0.571\nRecall:    0.511\nF1 Score:  0.539\nROC AUC:   0.808\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](03_modeling_files/figure-html/cell-7-output-2.png){width=543 height=468}\n:::\n\n::: {.cell-output .cell-output-display}\n![](03_modeling_files/figure-html/cell-7-output-3.png){width=470 height=467}\n:::\n:::\n\n\n# **Summary and exports**\n***\nAll preprocessing for this analysis was handled through a discrete pipeline, and the model was trained on a balanced dataset using SMOTE. \n\nThreshold tuning on the validation set was used to select a conservative cutoff optimized for F1 score.\n\nThe model generalizes well to unseen data, with only minimal performance drop from validation. The high precision confirms that threshold tuning reduced false positives.\n\nThe trained model, selected threshold, and test data are exported for a deeper look at the influence of individual features in the next notebook: `04_interpretation.ipynb`.\n\n::: {#7757d6c2 .cell quarto-private-1='{\"key\":\"jupyter\",\"value\":{\"source_hidden\":true}}' execution_count=7}\n\n::: {.cell-output .cell-output-stdout}\n```\nModel and threshold exported successfully.\nPreprocessing pipeline exported successfully.\n```\n:::\n:::\n\n\n",
    "supporting": [
      "03_modeling_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}