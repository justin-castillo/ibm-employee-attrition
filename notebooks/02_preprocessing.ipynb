{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b5cf274",
   "metadata": {},
   "source": [
    "# Cleaning and Preprocessing\n",
    "\n",
    "## 1. Notebook overview\n",
    "\n",
    "This notebook prepares the dataset for modeling by defining and exporting a reusable preprocessing pipeline.\n",
    "\n",
    "Specifically, it:\n",
    "\n",
    "- Applies custom feature engineering using a `FeatureEngineer` transformer.\n",
    "- Encodes ordinal and nominal categorical variables with `OrdinalEncoder` and `OneHotEncoder`.\n",
    "- Scales selected continuous features using `StandardScaler`.\n",
    "- Bundles all preprocessing steps into a scikit-learn `Pipeline`.\n",
    "- Serializes the complete pipeline using `joblib` for reuse in `03_modeling.ipynb`.\n",
    "\n",
    "No modeling or data splitting occurs in this notebook. All transformations are deferred to `03_modeling.ipynb`, which will load the saved pipeline and apply it to the data during training and evaluation.\n",
    "\n",
    "This is the second step in the pipeline following `01_eda.ipynb`, and it feeds directly into `03_modeling.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e44762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing environment initialized.\n"
     ]
    }
   ],
   "source": [
    "# Imports for preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')  \n",
    "from feature_engineering import FeatureEngineer\n",
    "\n",
    "# Set options for display\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Preprocessing environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6e0ee",
   "metadata": {},
   "source": [
    "## 2. Reload data and confirm schema\n",
    "\n",
    "We load the cleaned dataset exported from the EDA notebook (`data_01.csv`) and verify its structure before proceeding with preprocessing.\n",
    "\n",
    "This step ensures:\n",
    "- The dataset was saved correctly.\n",
    "- The schema matches expectations (column names, data types).\n",
    "- There are no unexpected missing values or type mismatches introduced during export.\n",
    "\n",
    "We also validate that the dataset includes exactly the expected columns — no more, no less. This prevents issues downstream if column names are altered, dropped, or duplicated.\n",
    "\n",
    "The expected schema includes only meaningful, cleaned features after removing non-informative columns in the EDA phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2015a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column schema validation passed.\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_csv('../data/processed/data_01.csv')\n",
    "\n",
    "# Define expected column names after EDA cleanup\n",
    "expected_columns = [\n",
    "    'Age', 'Attrition', 'BusinessTravel', 'DailyRate', 'Department',\n",
    "    'DistanceFromHome', 'Education', 'EducationField', 'EnvironmentSatisfaction',\n",
    "    'Gender', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole',\n",
    "    'JobSatisfaction', 'MaritalStatus', 'MonthlyIncome', 'MonthlyRate',\n",
    "    'NumCompaniesWorked', 'OverTime', 'PercentSalaryHike', 'PerformanceRating',\n",
    "    'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears',\n",
    "    'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany',\n",
    "    'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'\n",
    "]\n",
    "\n",
    "# Get actual columns from the loaded DataFrame\n",
    "actual_columns = list(df.columns)\n",
    "\n",
    "# Compare against expected\n",
    "missing_columns = set(expected_columns) - set(actual_columns)\n",
    "unexpected_columns = set(actual_columns) - set(expected_columns)\n",
    "\n",
    "# Display results\n",
    "if not missing_columns and not unexpected_columns:\n",
    "    print(\"Column schema validation passed.\")\n",
    "else:\n",
    "    if missing_columns:\n",
    "        print(\"Missing columns:\", missing_columns)\n",
    "    if unexpected_columns:\n",
    "        print(\"Unexpected columns:\", unexpected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e977d",
   "metadata": {},
   "source": [
    "## 3. Cleaning checks\n",
    "\n",
    "We perform additional cleaning checks on the dataset before continuing preprocessing.\n",
    "\n",
    "This includes:\n",
    "- Verifying data types are appropriate.\n",
    "- Checking for unexpected nulls (none should exist).\n",
    "- Ensuring no constant or identifier columns remain.\n",
    "- Confirming target class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "279be89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No null values found.\n",
      "\n",
      "Data types:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Age                          int64\n",
       "Attrition                   object\n",
       "BusinessTravel              object\n",
       "DailyRate                    int64\n",
       "Department                  object\n",
       "DistanceFromHome             int64\n",
       "Education                    int64\n",
       "EducationField              object\n",
       "EnvironmentSatisfaction      int64\n",
       "Gender                      object\n",
       "HourlyRate                   int64\n",
       "JobInvolvement               int64\n",
       "JobLevel                     int64\n",
       "JobRole                     object\n",
       "JobSatisfaction              int64\n",
       "MaritalStatus               object\n",
       "MonthlyIncome                int64\n",
       "MonthlyRate                  int64\n",
       "NumCompaniesWorked           int64\n",
       "OverTime                    object\n",
       "PercentSalaryHike            int64\n",
       "PerformanceRating            int64\n",
       "RelationshipSatisfaction     int64\n",
       "StockOptionLevel             int64\n",
       "TotalWorkingYears            int64\n",
       "TrainingTimesLastYear        int64\n",
       "WorkLifeBalance              int64\n",
       "YearsAtCompany               int64\n",
       "YearsInCurrentRole           int64\n",
       "YearsSinceLastPromotion      int64\n",
       "YearsWithCurrManager         int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No constant columns detected.\n",
      "\n",
      "Class balance in 'Attrition':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Attrition\n",
       "No     0.839\n",
       "Yes    0.161\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for nulls (none expected)\n",
    "null_counts = df.isnull().sum()\n",
    "if null_counts.any():\n",
    "    print(\"Unexpected null values found:\")\n",
    "    display(null_counts[null_counts > 0])\n",
    "else:\n",
    "    print(\"No null values found.\")\n",
    "\n",
    "# Recheck data types\n",
    "print(\"\\nData types:\")\n",
    "display(df.dtypes)\n",
    "\n",
    "# Identify constant columns (only one unique value)\n",
    "nunique = df.nunique()\n",
    "constant_cols = nunique[nunique == 1].index.tolist()\n",
    "\n",
    "if constant_cols:\n",
    "    print(f\"Constant columns detected and dropped: {constant_cols}\")\n",
    "    df.drop(columns=constant_cols, inplace=True)\n",
    "    print(f\"New shape after dropping: {df.shape}\")\n",
    "else:\n",
    "    print(\"No constant columns detected.\")\n",
    "\n",
    "# Confirm target variable distribution\n",
    "print(\"\\nClass balance in 'Attrition':\")\n",
    "display(df['Attrition'].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe07bf8",
   "metadata": {},
   "source": [
    "## 4. Feature engineering pipeline\n",
    "\n",
    "- This section creates custom features to capture patterns not directly visible in the raw data. \n",
    "  - It includes tenure ratios, satisfaction aggregates, income transformations, and interaction terms that reflect role dynamics, workload, and employee stability. \n",
    "  - These engineered features aim to enhance model performance by providing more expressive signals aligned with real-world attrition behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c88658",
   "metadata": {},
   "source": [
    "### Custom Feature Engineering\n",
    "\n",
    "The `FeatureEngineer` transformer creates domain-informed features designed to surface complex relationships in employee behavior, satisfaction, and tenure. These engineered variables aim to boost model signal by compressing nonlinear interactions and exposing patterns not easily captured by raw variables.\n",
    "\n",
    "Below is a detailed breakdown of each derived feature and the rationale for its inclusion:\n",
    "\n",
    "---\n",
    "\n",
    "### Tenure and Experience Features\n",
    "\n",
    "**`TenureCategory`**  \n",
    "Buckets `YearsAtCompany` into tenure groups:  \n",
    "- `0–3 yrs`  \n",
    "- `4–6 yrs`  \n",
    "- `7–10 yrs`  \n",
    "- `10+ yrs`  \n",
    "This captures key career stage segments, which may correspond to different attrition risks.\n",
    "\n",
    "**`TenureGap`**  \n",
    "Calculates: `YearsInCurrentRole` − `YearsAtCompany`  \n",
    "Surfaces employees who may have changed roles internally versus those who stayed static, potentially indicating engagement or stagnation.\n",
    "\n",
    "**`TenureRatio`**  \n",
    "Calculates: `YearsInCurrentRole` / `YearsAtCompany`  \n",
    "Normalizes role tenure by company tenure to identify fast or slow transitions. High ratios may indicate stagnation, while low ratios may indicate fast promotions or instability.\n",
    "\n",
    "**`ZeroCompanyTenureFlag`**  \n",
    "Binary flag indicating `YearsAtCompany` == 0  \n",
    "Captures newly joined employees who may behave differently or lack long-term integration.\n",
    "\n",
    "**`NewJoinerFlag`**  \n",
    "Flags employees with:\n",
    "- `YearsAtCompany` < 2  \n",
    "- `TotalWorkingYears` > 3  \n",
    "These are experienced professionals recently joining, which may suggest job-hopping or career instability.\n",
    "\n",
    "---\n",
    "\n",
    "### Role and Work Simplifications\n",
    "\n",
    "**`JobRole_Simplified`**  \n",
    "Collapses job roles into broader categories:  \n",
    "- “Technical” vs. “Other”  \n",
    "Highlights differences in attrition patterns between technical vs. non-technical roles.\n",
    "\n",
    "**`Overtime_JobLevel`**  \n",
    "Encodes interaction between `OverTime` and `JobLevel`  \n",
    "Useful for identifying high-level staff taking overtime (possible burnout) or junior staff being overworked.\n",
    "\n",
    "**`Travel_Occupation`**  \n",
    "Encodes combined effect of travel status and job role.  \n",
    "Can surface role-travel patterns associated with higher attrition.\n",
    "\n",
    "---\n",
    "\n",
    "### Satisfaction Features\n",
    "\n",
    "**`SatisfactionMean`**  \n",
    "Averages the 3 key satisfaction scores:  \n",
    "- `EnvironmentSatisfaction`  \n",
    "- `JobSatisfaction`  \n",
    "- `RelationshipSatisfaction`  \n",
    "Provides a generalized view of employee sentiment.\n",
    "\n",
    "**`SatisfactionRange`**  \n",
    "Calculates: max − min of the 3 satisfaction scores  \n",
    "Surfaces inconsistency or volatility in perceived satisfaction, potentially indicating internal conflict or instability.\n",
    "\n",
    "**`SatisfactionStability`**  \n",
    "Binary flag: 1 if all 3 satisfaction scores are equal  \n",
    "Identifies employees with consistent satisfaction levels across all domains.\n",
    "\n",
    "---\n",
    "\n",
    "### Financial Features\n",
    "\n",
    "**`Log_MonthlyIncome`**  \n",
    "Applies log transform to `MonthlyIncome`  \n",
    "Reduces skew, stabilizes model learning, and compresses extreme values.\n",
    "\n",
    "**`Log_DistanceFromHome`**  \n",
    "Applies log transform to `DistanceFromHome`  \n",
    "Captures diminishing marginal effect of commute distance on attrition.\n",
    "\n",
    "**`LowIncomeFlag`**  \n",
    "Binary flag for employees earning below the 25th percentile of income  \n",
    "Captures financial dissatisfaction and potential disengagement.\n",
    "\n",
    "---\n",
    "\n",
    "### Composite Burnout Risk\n",
    "\n",
    "**`StressRisk`**  \n",
    "Binary flag for employees where:  \n",
    "- `OverTime` == Yes  \n",
    "- `JobSatisfaction` ≤ 2  \n",
    "- `SatisfactionMean` < 2.5  \n",
    "Combines workload and dissatisfaction into a high-risk signal for possible voluntary attrition.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c70b1cf",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline definition \n",
    "\n",
    "- Ordinal variables are encoded with ordered categories to preserve their scale. Nominal categorical features are one-hot encoded. \n",
    "- Continuous numerical features are standardized to ensure uniform scale. Binary flags are passed through unchanged. \n",
    "- The `ColumnTransformer` bundles all transformations, and a full `Pipeline` integrates both custom feature engineering and preprocessing for consistent application during training and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd53c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "def make_preprocessing_pipeline():\n",
    "    # Ordinal encoding map\n",
    "    ordinal_cols = ['EnvironmentSatisfaction', 'JobSatisfaction', 'RelationshipSatisfaction', 'WorkLifeBalance']\n",
    "    ordinal_map = [['1', '2', '3', '4']] * len(ordinal_cols)\n",
    "\n",
    "    # Nominal (OneHot) encoded categorical features\n",
    "    nominal_cols = [\n",
    "        'BusinessTravel', 'Department', 'EducationField', 'Gender', 'MaritalStatus', 'OverTime',\n",
    "        'JobRole_Simplified', 'TenureCategory', 'OverTime_JobLevel', 'Travel_Occupation'\n",
    "    ]\n",
    "\n",
    "    # Scaled numeric features (original + engineered)\n",
    "    scale_cols = [\n",
    "        'Age', 'DistanceFromHome', 'HourlyRate', 'JobInvolvement', 'JobLevel',\n",
    "        'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating',\n",
    "        'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany',\n",
    "        'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager',\n",
    "        'TenureRatio', 'TenureGap', 'SatisfactionMean', 'SatisfactionRange',\n",
    "        'PromotionPerYear', 'YearsCompany_Satisfaction',\n",
    "        'Log_MonthlyIncome', 'Log_DistanceFromHome'\n",
    "    ]\n",
    "\n",
    "    # Binary passthrough flags\n",
    "    passthrough_cols = [\n",
    "        'ZeroCompanyTenureFlag', 'NewJoinerFlag', 'LowIncomeFlag',\n",
    "        'SatisfactionStability', 'StressRisk'\n",
    "    ]\n",
    "\n",
    "    # Build column transformer\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('ordinal', OrdinalEncoder(categories=ordinal_map), ordinal_cols),\n",
    "        ('nominal', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), nominal_cols),\n",
    "        ('scale', StandardScaler(), scale_cols),\n",
    "        ('passthrough', 'passthrough', passthrough_cols)\n",
    "    ])\n",
    "\n",
    "    # Wrap with full pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('feature_engineering', FeatureEngineer()),\n",
    "        ('preprocessing', preprocessor)\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e3ad9e",
   "metadata": {},
   "source": [
    "### Fit and export pipeline \n",
    "\n",
    "- The preprocessing pipeline is instantiated and fitted on the cleaned dataset, excluding the target column `Attrition` to avoid data leakage. \n",
    "- After fitting, the pipeline is serialized using `joblib` and saved to disk. This allows the same preprocessing steps to be consistently reused during model inference and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "750f4f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pipeline saved.\n"
     ]
    }
   ],
   "source": [
    "# Fit on cleaned data (exclude target)\n",
    "X_clean = df.drop(columns='Attrition')\n",
    "pipeline = make_preprocessing_pipeline()\n",
    "pipeline.fit(X_clean)\n",
    "\n",
    "# Save pipeline\n",
    "joblib.dump(pipeline, '../models/preprocessing_pipeline.pkl')\n",
    "print(\"Preprocessing pipeline saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2680ef",
   "metadata": {},
   "source": [
    "## Preprocessing summary and next steps\n",
    "\n",
    "This notebook implemented the full preprocessing pipeline for the attrition dataset, including:\n",
    "\n",
    "- Custom feature engineering via a `FeatureEngineer` transformer\n",
    "- Encoding of ordinal and nominal categorical features using `OrdinalEncoder` and `OneHotEncoder`\n",
    "- Standardization of selected continuous variables using `StandardScaler`\n",
    "- Integration of all steps into a scikit-learn `Pipeline` for modular reuse\n",
    "- Exporting the complete pipeline to `../models/preprocessing_pipeline.pkl` using `joblib`\n",
    "\n",
    "The exported pipeline preserves all transformations and ensures consistent preprocessing during model training, evaluation, and deployment. It can be loaded into `modeling.ipynb` to prevent data leakage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm-attrition-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
