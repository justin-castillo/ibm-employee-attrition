{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b5cf274",
   "metadata": {},
   "source": [
    "# Cleaning and Preprocessing\n",
    "\n",
    "## 1. Notebook overview\n",
    "\n",
    "This notebook prepares the dataset for modeling by performing data cleaning, feature engineering, and preprocessing.\n",
    "\n",
    "Specifically, it:\n",
    "- Reloads the cleaned dataset from the previous EDA step.\n",
    "- Applies cleaning and formatting for consistency.\n",
    "- Encodes categorical variables appropriately.\n",
    "- Scales numerical features if needed.\n",
    "- Handles class imbalance in the target variable.\n",
    "- Splits the data into training and test sets.\n",
    "- Exports the preprocessed dataset for modeling.\n",
    "\n",
    "This is the second step in the pipeline following `01_eda.ipynb`, and it feeds into `03_modeling.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e44762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing environment initialized.\n"
     ]
    }
   ],
   "source": [
    "# Imports for preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Set options for display\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Preprocessing environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6e0ee",
   "metadata": {},
   "source": [
    "## 2. Reload data and confirm schema\n",
    "\n",
    "We load the cleaned dataset exported from the EDA notebook (`data_01.csv`) and verify its structure before proceeding with preprocessing.\n",
    "\n",
    "This step ensures:\n",
    "- The dataset was saved correctly.\n",
    "- The schema matches expectations (column names, data types).\n",
    "- There are no unexpected missing values or type mismatches introduced during export.\n",
    "\n",
    "We also validate that the dataset includes exactly the expected columns — no more, no less. This prevents issues downstream if column names are altered, dropped, or duplicated.\n",
    "\n",
    "The expected schema includes only meaningful, cleaned features after removing non-informative columns in the EDA phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2015a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column schema validation passed.\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_csv('../data/processed/data_01.csv')\n",
    "\n",
    "# Define expected column names after EDA cleanup\n",
    "expected_columns = [\n",
    "    'Age', 'Attrition', 'BusinessTravel', 'DailyRate', 'Department',\n",
    "    'DistanceFromHome', 'Education', 'EducationField', 'EnvironmentSatisfaction',\n",
    "    'Gender', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole',\n",
    "    'JobSatisfaction', 'MaritalStatus', 'MonthlyIncome', 'MonthlyRate',\n",
    "    'NumCompaniesWorked', 'OverTime', 'PercentSalaryHike', 'PerformanceRating',\n",
    "    'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears',\n",
    "    'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany',\n",
    "    'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'\n",
    "]\n",
    "\n",
    "# Get actual columns from the loaded DataFrame\n",
    "actual_columns = list(df.columns)\n",
    "\n",
    "# Compare against expected\n",
    "missing_columns = set(expected_columns) - set(actual_columns)\n",
    "unexpected_columns = set(actual_columns) - set(expected_columns)\n",
    "\n",
    "# Display results\n",
    "if not missing_columns and not unexpected_columns:\n",
    "    print(\"Column schema validation passed.\")\n",
    "else:\n",
    "    if missing_columns:\n",
    "        print(\"Missing columns:\", missing_columns)\n",
    "    if unexpected_columns:\n",
    "        print(\"Unexpected columns:\", unexpected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e977d",
   "metadata": {},
   "source": [
    "## 3. Cleaning checks\n",
    "\n",
    "We perform additional cleaning checks on the dataset before continuing preprocessing.\n",
    "\n",
    "This includes:\n",
    "- Verifying data types are appropriate.\n",
    "- Checking for unexpected nulls (none should exist).\n",
    "- Ensuring no constant or identifier columns remain.\n",
    "- Confirming target class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279be89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No null values found.\n",
      "\n",
      "Data types:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Age                          int64\n",
       "Attrition                   object\n",
       "BusinessTravel              object\n",
       "DailyRate                    int64\n",
       "Department                  object\n",
       "DistanceFromHome             int64\n",
       "Education                    int64\n",
       "EducationField              object\n",
       "EnvironmentSatisfaction      int64\n",
       "Gender                      object\n",
       "HourlyRate                   int64\n",
       "JobInvolvement               int64\n",
       "JobLevel                     int64\n",
       "JobRole                     object\n",
       "JobSatisfaction              int64\n",
       "MaritalStatus               object\n",
       "MonthlyIncome                int64\n",
       "MonthlyRate                  int64\n",
       "NumCompaniesWorked           int64\n",
       "OverTime                    object\n",
       "PercentSalaryHike            int64\n",
       "PerformanceRating            int64\n",
       "RelationshipSatisfaction     int64\n",
       "StockOptionLevel             int64\n",
       "TotalWorkingYears            int64\n",
       "TrainingTimesLastYear        int64\n",
       "WorkLifeBalance              int64\n",
       "YearsAtCompany               int64\n",
       "YearsInCurrentRole           int64\n",
       "YearsSinceLastPromotion      int64\n",
       "YearsWithCurrManager         int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No constant columns detected.\n",
      "\n",
      "Class balance in 'Attrition':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Attrition\n",
       "No     0.839\n",
       "Yes    0.161\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for nulls (none expected)\n",
    "null_counts = df.isnull().sum()\n",
    "if null_counts.any():\n",
    "    print(\"Unexpected null values found:\")\n",
    "    display(null_counts[null_counts > 0])\n",
    "else:\n",
    "    print(\"No null values found.\")\n",
    "\n",
    "# Recheck data types\n",
    "print(\"\\nData types:\")\n",
    "display(df.dtypes)\n",
    "\n",
    "# Identify constant columns (only one unique value)\n",
    "nunique = df.nunique()\n",
    "constant_cols = nunique[nunique == 1].index.tolist()\n",
    "\n",
    "if constant_cols:\n",
    "    print(f\"Constant columns detected and dropped: {constant_cols}\")\n",
    "    df.drop(columns=constant_cols, inplace=True)\n",
    "    print(f\"New shape after dropping: {df.shape}\")\n",
    "else:\n",
    "    print(\"No constant columns detected.\")\n",
    "\n",
    "# Confirm target variable distribution\n",
    "print(\"\\nClass balance in 'Attrition':\")\n",
    "display(df['Attrition'].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a7660a",
   "metadata": {},
   "source": [
    "## 4. Feature engineering\n",
    "\n",
    "In this section, we enhance the feature space with domain-informed transformations and interactions that may improve model performance.\n",
    "\n",
    "We include:\n",
    "\n",
    "### Tenure-related features\n",
    "- `TenureCategory`: Bins `YearsAtCompany` into ranges to capture non-linear tenure patterns.\n",
    "- `YearsAtCompanyRatio`: Fraction of career spent at current company.\n",
    "- `YearsInRoleRatio`: Stability in the same role.\n",
    "- `YearsWithManagerRatio`: Indicates long-term relationship with supervisor.\n",
    "\n",
    "### Income-related features\n",
    "- `IncomePerYear`: Ratio of monthly income to tenure.\n",
    "- `HighIncomeShortTenure`: Flags early high earners (possible burnout risk).\n",
    "- `HighIncomeFlag`: Binary flag for income above median.\n",
    "- `HourlyRateBucket`: Bucketized hourly pay to reduce skew.\n",
    "- `IncomePerEduLevel`: Normalized income by education level.\n",
    "- `IncomeZScoreWithinEdu`: Z-score of income within each education level group.\n",
    "\n",
    "### Job role and travel groupings\n",
    "- `JobRoleGroup`: Simplifies rare job roles into broader categories.\n",
    "- `Travel_Occupation`: Interaction between business travel frequency and job role.\n",
    "\n",
    "### Satisfaction-based signals\n",
    "- `SatisfactionAvg`: Mean satisfaction across 3 dimensions.\n",
    "- `SatisfactionRange`: Range between highest and lowest satisfaction.\n",
    "- `LowSatisfactionFlag`: Any satisfaction dimension rated ≤ 2.\n",
    "\n",
    "### Interaction and composite flags\n",
    "- `OverTime_JobLevel`: Combines overtime flag with job level.\n",
    "- `StressRisk`: Composite risk index based on overtime, commute, and satisfaction.\n",
    "\n",
    "These features are designed to expose nonlinear effects, interactions, and latent variables the model might otherwise miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All engineered features added.\n"
     ]
    }
   ],
   "source": [
    "# Tenure-based Features\n",
    "df['TenureCategory'] = pd.cut(\n",
    "    df['YearsAtCompany'],\n",
    "    bins=[-1, 2, 5, 10, np.inf],\n",
    "    labels=['<3 yrs', '3-5 yrs', '6-10 yrs', '10+ yrs']\n",
    ")\n",
    "\n",
    "df['YearsAtCompanyRatio'] = df['YearsAtCompany'] / df['TotalWorkingYears'].replace(0, np.nan) # Avoids division by 0 - nulls created here are handled in the next cell\n",
    "df['YearsInRoleRatio'] = df['YearsInCurrentRole'] / df['YearsAtCompany'].replace(0, np.nan)\n",
    "df['YearsWithManagerRatio'] = df['YearsWithCurrManager'] / df['YearsAtCompany'].replace(0, np.nan)\n",
    "\n",
    "# Income-based Features\n",
    "df['IncomePerYear'] = df['MonthlyIncome'] / df['YearsAtCompany'].replace(0, np.nan)\n",
    "df['HighIncomeShortTenure'] = (\n",
    "    (df['MonthlyIncome'] > df['MonthlyIncome'].median()) &\n",
    "    (df['YearsAtCompany'] < 3)\n",
    ").astype(int)\n",
    "df['HighIncomeFlag'] = (df['MonthlyIncome'] > df['MonthlyIncome'].median()).astype(int)\n",
    "\n",
    "df['HourlyRateBucket'] = pd.qcut(df['HourlyRate'], q=4, labels=False)\n",
    "df['IncomePerEduLevel'] = df['MonthlyIncome'] / df['Education'].replace(0, np.nan)\n",
    "\n",
    "df['IncomeZScoreWithinEdu'] = df.groupby('Education')['MonthlyIncome'].transform(\n",
    "    lambda x: (x - x.mean()) / x.std(ddof=0) # Population standard deviation; population = all employees at each education level; avoids dividing by 0\n",
    ")\n",
    "\n",
    "# Job Role Groupings\n",
    "df['JobRoleGroup'] = df['JobRole'].replace({\n",
    "    'Laboratory Technician': 'Technical',\n",
    "    'Research Scientist': 'Technical',\n",
    "    'Healthcare Representative': 'Sales',\n",
    "    'Sales Executive': 'Sales',\n",
    "    'Sales Representative': 'Sales'\n",
    "})\n",
    "\n",
    "# Satisfaction Features\n",
    "df['SatisfactionAvg'] = df[[\n",
    "    'EnvironmentSatisfaction', 'JobSatisfaction', 'RelationshipSatisfaction'\n",
    "]].mean(axis=1)\n",
    "\n",
    "df['SatisfactionRange'] = df[[\n",
    "    'EnvironmentSatisfaction', 'JobSatisfaction', 'RelationshipSatisfaction'\n",
    "]].max(axis=1) - df[[\n",
    "    'EnvironmentSatisfaction', 'JobSatisfaction', 'RelationshipSatisfaction'\n",
    "]].min(axis=1)\n",
    "\n",
    "df['LowSatisfactionFlag'] = (\n",
    "    (df['EnvironmentSatisfaction'] <= 2) |\n",
    "    (df['JobSatisfaction'] <= 2) |\n",
    "    (df['RelationshipSatisfaction'] <= 2)\n",
    ").astype(int)\n",
    "\n",
    "# Interaction Features\n",
    "df['OverTime_JobLevel'] = df['OverTime'].astype(str) + \"_\" + df['JobLevel'].astype(str)\n",
    "df['Travel_Occupation'] = df['BusinessTravel'].astype(str) + \"_\" + df['JobRole'].astype(str)\n",
    "\n",
    "# Composite Risk Flag\n",
    "df['StressRisk'] = (\n",
    "    (df['OverTime'] == 'Yes') &\n",
    "    (df['DistanceFromHome'] > 10) &\n",
    "    (df['SatisfactionAvg'] < 2.5)\n",
    ").astype(int)\n",
    "\n",
    "print(\"All engineered features added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fd78f",
   "metadata": {},
   "source": [
    "#### Impute 0 and flag employees with no work experience\n",
    "- If both YearsAtCompany and TotalWorkingYears are 0, this implies the employee is new to the workforce (fresh grad). \n",
    "- While there are only 11 cases of this, it's important to include data from these instances as they reflect a unique first impression of all features in the dataset. \n",
    "- `NoWorkHistoryFlag` allows the model to easily identify these cases. \n",
    "- `YearsAtCompanyRatio` is imputed with 0 as an accurate way to get rid of the nulls created earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f0daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YearsAtCompanyRatio\n",
       "1.000000    463\n",
       "0.500000     59\n",
       "0.833333     55\n",
       "0.000000     44\n",
       "0.800000     41\n",
       "           ... \n",
       "0.863636      1\n",
       "0.722222      1\n",
       "0.034483      1\n",
       "0.680000      1\n",
       "0.529412      1\n",
       "Name: count, Length: 192, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['YearsAtCompanyRatio'] = df['YearsAtCompanyRatio'].fillna(0)\n",
    "\n",
    "df['YearsAtCompanyRatio'].value_counts()\n",
    "\n",
    "df['NoWorkHistoryFlag'] = ((df['YearsAtCompany'] == 0) & (df['TotalWorkingYears'] == 0)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30218050",
   "metadata": {},
   "source": [
    "## 5. Encoding categorical variables\n",
    "\n",
    "To prepare the dataset for modeling, we encode categorical variables into numerical form.\n",
    "\n",
    "We apply:\n",
    "- **Label encoding** for binary and ordinal variables (e.g., `Attrition`, `TenureCategory`)\n",
    "- **One-hot encoding** for nominal variables with more than 2 categories\n",
    "\n",
    "This transformation ensures that all features are numeric, as required by most machine learning models. We also drop one dummy column per one-hot-encoded feature to avoid multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25824cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "\n",
    "# Make a copy to work from\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# --- Binary Label Encoding ---\n",
    "binary_cols = [\n",
    "    'Attrition', 'OverTime', 'Gender', 'HighIncomeFlag',\n",
    "    'LowSatisfactionFlag', 'HighIncomeShortTenure', 'StressRisk'\n",
    "]\n",
    "\n",
    "for col in binary_cols:\n",
    "    df_encoded[col] = LabelEncoder().fit_transform(df_encoded[col])\n",
    "\n",
    "# --- Ordinal Encoding ---\n",
    "df_encoded['TenureCategory'] = df_encoded['TenureCategory'].map({\n",
    "    '<3 yrs': 0,\n",
    "    '3-5 yrs': 1,\n",
    "    '6-10 yrs': 2,\n",
    "    '10+ yrs': 3\n",
    "})\n",
    "\n",
    "# --- One-hot Encoding ---\n",
    "nominal_cols = [\n",
    "    'BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus',\n",
    "    'JobRoleGroup', 'Travel_Occupation', 'OverTime_JobLevel'\n",
    "]\n",
    "\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=nominal_cols, drop_first=True)\n",
    "\n",
    "print(\"Categorical variables encoded.\")\n",
    "print(f\"Encoded shape: {df_encoded.shape}\")\n",
    "\n",
    "# --- Inspect new columns ---\n",
    "original_cols = set(df.columns)\n",
    "encoded_cols = set(df_encoded.columns)\n",
    "new_columns = sorted(encoded_cols - original_cols)\n",
    "\n",
    "if new_columns:\n",
    "    print(f\"\\n🆕 {len(new_columns)} new columns added via one-hot encoding:\\n\")\n",
    "    for col in new_columns:\n",
    "        print(f\"  {col}\")\n",
    "else:\n",
    "    print(\"No new columns were added via one-hot encoding.\")\n",
    "\n",
    "# --- Group one-hot columns by source ---\n",
    "grouped_dummies = defaultdict(list)\n",
    "for col in new_columns:\n",
    "    for original in nominal_cols:\n",
    "        if col.startswith(original + \"_\"):\n",
    "            grouped_dummies[original].append(col)\n",
    "\n",
    "# --- Display grouped dummy columns ---\n",
    "for key in sorted(grouped_dummies):\n",
    "    values = grouped_dummies[key]\n",
    "    print(f\"\\n{key} → {len(values)} dummy columns:\")\n",
    "    for v in sorted(values):\n",
    "        print(f\"  {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c880a",
   "metadata": {},
   "source": [
    "### Removing rare dummy variables\n",
    "\n",
    "Some one-hot encoded columns may represent categories with very few samples, which can introduce instability during training. We drop dummy variables that appear in fewer than 10 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600bce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define minimum threshold\n",
    "min_count = 10\n",
    "\n",
    "# Identify dummy columns that were just created\n",
    "original_cols = set(df.columns)\n",
    "encoded_cols = set(df_encoded.columns)\n",
    "new_dummies = sorted(encoded_cols - original_cols)\n",
    "\n",
    "# Check frequency of each dummy column\n",
    "rare_dummies = [col for col in new_dummies if df_encoded[col].sum() < min_count]\n",
    "\n",
    "# Drop and report\n",
    "if rare_dummies:\n",
    "    df_encoded.drop(columns=rare_dummies, inplace=True)\n",
    "    print(f\"Dropped {len(rare_dummies)} rare dummy columns (fewer than {min_count} observations):\")\n",
    "    for col in rare_dummies:\n",
    "        print(f\"  {col}\")\n",
    "else:\n",
    "    print(\"No rare dummy columns to drop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb6636d",
   "metadata": {},
   "source": [
    "## 6. Feature scaling\n",
    "\n",
    "Many machine learning models are sensitive to the scale of input features, especially distance-based or regularized models.\n",
    "\n",
    "In this step, we:\n",
    "- Identify numerical features that are not binary or categorical\n",
    "- Apply `StandardScaler` to normalize those features\n",
    "- Leave tree-based models unaffected (e.g., XGBoost, Random Forest), but ensure consistency if used in ensemble or linear models\n",
    "\n",
    "Note: Scaling is not strictly required for decision trees or ensembles, but we apply it for flexibility and consistency across modeling approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d7d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Identify columns to scale: numerical, not binary, not target\n",
    "numeric_cols = df_encoded.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Exclude binary flags and target column\n",
    "exclude = [\n",
    "    'Attrition',  # Target\n",
    "    'HighIncomeFlag', 'LowSatisfactionFlag',\n",
    "    'HighIncomeShortTenure', 'StressRisk'\n",
    "]\n",
    "\n",
    "# Keep only those that are continuous\n",
    "scale_cols = [col for col in numeric_cols if col not in exclude]\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply scaling\n",
    "df_encoded[scale_cols] = scaler.fit_transform(df_encoded[scale_cols])\n",
    "\n",
    "print(f\"Scaled {len(scale_cols)} numeric features.\")\n",
    "print(\"Scaled columns:\")\n",
    "for col in scale_cols:\n",
    "    print(f\"  {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0729af",
   "metadata": {},
   "source": [
    "### Histogram comparison: Before and after scaling\n",
    "\n",
    "We plot the distribution of selected numeric features before and after scaling. This helps verify that scaling transformed the features to standard normal (mean ≈ 0, std ≈ 1), while preserving their shape and distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Prepare original and scaled versions\n",
    "df_unscaled = df.copy()\n",
    "df_scaled = df_encoded.copy()\n",
    "\n",
    "# Set up grid\n",
    "num_features = len(scale_cols)\n",
    "cols_per_row = 2  # Original vs scaled\n",
    "rows = num_features\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols_per_row, figsize=(12, rows * 3))\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "for i, col in enumerate(scale_cols):\n",
    "    # Original\n",
    "    sns.histplot(df_unscaled[col], kde=True, ax=axes[i, 0], bins=30, color='skyblue')\n",
    "    axes[i, 0].set_title(f\"Original: {col}\")\n",
    "    \n",
    "    # Scaled\n",
    "    sns.histplot(df_scaled[col], kde=True, ax=axes[i, 1], bins=30, color='salmon')\n",
    "    axes[i, 1].set_title(f\"Scaled: {col}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Before vs After Scaling (All Numeric Features)\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a44eb",
   "metadata": {},
   "source": [
    "## 7. Train-test split\n",
    "\n",
    "We split the dataset into training and testing sets to evaluate generalization.\n",
    "\n",
    "Key considerations:\n",
    "- We use a stratified split on the target (`Attrition`) to preserve class balance.\n",
    "- 80% of the data is used for training; 20% is held out for testing.\n",
    "- Random seed is set for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define feature matrix and target\n",
    "X = df_encoded.drop(columns='Attrition')\n",
    "y = df_encoded['Attrition']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Confirm shape and class balance\n",
    "print(f\"Split complete:\")\n",
    "print(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"  X_test:  {X_test.shape}, y_test:  {y_test.shape}\")\n",
    "\n",
    "print(\"\\nClass distribution in y_train:\")\n",
    "display(y_train.value_counts(normalize=True).round(3))\n",
    "\n",
    "print(\"Class distribution in y_test:\")\n",
    "display(y_test.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d8c57",
   "metadata": {},
   "source": [
    "## 8. Handling class imbalance\n",
    "\n",
    "The target variable `Attrition` is imbalanced, with far more `No` than `Yes` cases.\n",
    "\n",
    "To address this:\n",
    "- We apply **SMOTE (Synthetic Minority Oversampling Technique)** to the training data.\n",
    "- SMOTE generates synthetic examples of the minority class by interpolating between existing ones.\n",
    "- We apply it **only to the training set** to avoid information leakage.\n",
    "\n",
    "This step is useful when using models sensitive to class imbalance (e.g., logistic regression, SVM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=RANDOM_STATE)\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Confirm result\n",
    "print(\"SMOTE applied to training data.\")\n",
    "print(f\"  X_train before: {X_train.shape}, after: {X_train_resampled.shape}\")\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "display(y_train_resampled.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e487b",
   "metadata": {},
   "source": [
    "## 9. Export preprocessed dataset\n",
    "\n",
    "We export the resampled and original datasets to the `../data/processed/` directory for use in the modeling phase.\n",
    "\n",
    "Saved files:\n",
    "- `X_train.csv`, `y_train.csv` — raw stratified training set\n",
    "- `X_test.csv`, `y_test.csv` — untouched test set\n",
    "- `X_train_resampled.csv`, `y_train_resampled.csv` — SMOTE-balanced training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b7e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"../data/processed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Export each component safely\n",
    "exports = {\n",
    "    \"X_train.csv\": X_train,\n",
    "    \"y_train.csv\": y_train,\n",
    "    \"X_test.csv\": X_test,\n",
    "    \"y_test.csv\": y_test,\n",
    "    \"X_train_resampled.csv\": X_train_resampled,\n",
    "    \"y_train_resampled.csv\": y_train_resampled\n",
    "}\n",
    "\n",
    "for filename, df_to_export in exports.items():\n",
    "    try:\n",
    "        full_path = os.path.join(output_dir, filename)\n",
    "        df_to_export.to_csv(full_path, index=False)\n",
    "        print(f\"✅ Exported: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to export {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60caf062",
   "metadata": {},
   "source": [
    "# Preprocessing Summary and Next Steps\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "- Reloaded and validated the cleaned dataset (`data_01.csv`)\n",
    "- Performed final data integrity checks (nulls, constants, schema)\n",
    "- Engineered domain-relevant features (e.g., tenure ratios, satisfaction flags)\n",
    "- Encoded categorical variables using label encoding and one-hot encoding\n",
    "- Scaled numeric features to standard normal\n",
    "- Applied a stratified train-test split to preserve class distribution\n",
    "- Handled class imbalance using SMOTE on the training set\n",
    "- Exported all relevant datasets for downstream modeling\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps (Modeling Phase)\n",
    "In the next notebook (`03_modeling.ipynb`), we will:\n",
    "\n",
    "- Load the preprocessed datasets\n",
    "- Train baseline classification models (Logistic Regression, Random Forest, etc.)\n",
    "- Evaluate using accuracy, recall, precision, F1-score, and ROC AUC\n",
    "- Tune hyperparameters and compare model performance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm-attrition-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
