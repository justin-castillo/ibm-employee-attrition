{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b5cf274",
   "metadata": {},
   "source": [
    "# Cleaning and Preprocessing\n",
    "\n",
    "## 1. Notebook overview\n",
    "\n",
    "This notebook prepares the dataset for modeling by defining and exporting a reusable preprocessing pipeline.\n",
    "\n",
    "Specifically, it:\n",
    "\n",
    "- Applies custom feature engineering using a `FeatureEngineer` transformer.\n",
    "- Encodes ordinal and nominal categorical variables with `OrdinalEncoder` and `OneHotEncoder`.\n",
    "- Scales selected continuous features using `StandardScaler`.\n",
    "- Bundles all preprocessing steps into a scikit-learn `Pipeline`.\n",
    "- Serializes the complete pipeline using `joblib` for reuse in `03_modeling.ipynb`.\n",
    "\n",
    "No modeling or data splitting occurs in this notebook. All transformations are deferred to `03_modeling.ipynb`, which will load the saved pipeline and apply it to the data during training and evaluation.\n",
    "\n",
    "This is the second step in the pipeline following `01_eda.ipynb`, and it feeds directly into `03_modeling.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e44762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing environment initialized.\n"
     ]
    }
   ],
   "source": [
    "# Imports for preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# Set options for display\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Preprocessing environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6e0ee",
   "metadata": {},
   "source": [
    "## 2. Reload data and confirm schema\n",
    "\n",
    "We load the cleaned dataset exported from the EDA notebook (`data_01.csv`) and verify its structure before proceeding with preprocessing.\n",
    "\n",
    "This step ensures:\n",
    "- The dataset was saved correctly.\n",
    "- The schema matches expectations (column names, data types).\n",
    "- There are no unexpected missing values or type mismatches introduced during export.\n",
    "\n",
    "We also validate that the dataset includes exactly the expected columns — no more, no less. This prevents issues downstream if column names are altered, dropped, or duplicated.\n",
    "\n",
    "The expected schema includes only meaningful, cleaned features after removing non-informative columns in the EDA phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2015a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column schema validation passed.\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_csv('../data/processed/data_01.csv')\n",
    "\n",
    "# Define expected column names after EDA cleanup\n",
    "expected_columns = [\n",
    "    'Age', 'Attrition', 'BusinessTravel', 'DailyRate', 'Department',\n",
    "    'DistanceFromHome', 'Education', 'EducationField', 'EnvironmentSatisfaction',\n",
    "    'Gender', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole',\n",
    "    'JobSatisfaction', 'MaritalStatus', 'MonthlyIncome', 'MonthlyRate',\n",
    "    'NumCompaniesWorked', 'OverTime', 'PercentSalaryHike', 'PerformanceRating',\n",
    "    'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears',\n",
    "    'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany',\n",
    "    'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'\n",
    "]\n",
    "\n",
    "# Get actual columns from the loaded DataFrame\n",
    "actual_columns = list(df.columns)\n",
    "\n",
    "# Compare against expected\n",
    "missing_columns = set(expected_columns) - set(actual_columns)\n",
    "unexpected_columns = set(actual_columns) - set(expected_columns)\n",
    "\n",
    "# Display results\n",
    "if not missing_columns and not unexpected_columns:\n",
    "    print(\"Column schema validation passed.\")\n",
    "else:\n",
    "    if missing_columns:\n",
    "        print(\"Missing columns:\", missing_columns)\n",
    "    if unexpected_columns:\n",
    "        print(\"Unexpected columns:\", unexpected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e977d",
   "metadata": {},
   "source": [
    "## 3. Cleaning checks\n",
    "\n",
    "We perform additional cleaning checks on the dataset before continuing preprocessing.\n",
    "\n",
    "This includes:\n",
    "- Verifying data types are appropriate.\n",
    "- Checking for unexpected nulls (none should exist).\n",
    "- Ensuring no constant or identifier columns remain.\n",
    "- Confirming target class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "279be89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No null values found.\n",
      "\n",
      "Data types:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Age                          int64\n",
       "Attrition                   object\n",
       "BusinessTravel              object\n",
       "DailyRate                    int64\n",
       "Department                  object\n",
       "DistanceFromHome             int64\n",
       "Education                    int64\n",
       "EducationField              object\n",
       "EnvironmentSatisfaction      int64\n",
       "Gender                      object\n",
       "HourlyRate                   int64\n",
       "JobInvolvement               int64\n",
       "JobLevel                     int64\n",
       "JobRole                     object\n",
       "JobSatisfaction              int64\n",
       "MaritalStatus               object\n",
       "MonthlyIncome                int64\n",
       "MonthlyRate                  int64\n",
       "NumCompaniesWorked           int64\n",
       "OverTime                    object\n",
       "PercentSalaryHike            int64\n",
       "PerformanceRating            int64\n",
       "RelationshipSatisfaction     int64\n",
       "StockOptionLevel             int64\n",
       "TotalWorkingYears            int64\n",
       "TrainingTimesLastYear        int64\n",
       "WorkLifeBalance              int64\n",
       "YearsAtCompany               int64\n",
       "YearsInCurrentRole           int64\n",
       "YearsSinceLastPromotion      int64\n",
       "YearsWithCurrManager         int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No constant columns detected.\n",
      "\n",
      "Class balance in 'Attrition':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Attrition\n",
       "No     0.839\n",
       "Yes    0.161\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for nulls (none expected)\n",
    "null_counts = df.isnull().sum()\n",
    "if null_counts.any():\n",
    "    print(\"Unexpected null values found:\")\n",
    "    display(null_counts[null_counts > 0])\n",
    "else:\n",
    "    print(\"No null values found.\")\n",
    "\n",
    "# Recheck data types\n",
    "print(\"\\nData types:\")\n",
    "display(df.dtypes)\n",
    "\n",
    "# Identify constant columns (only one unique value)\n",
    "nunique = df.nunique()\n",
    "constant_cols = nunique[nunique == 1].index.tolist()\n",
    "\n",
    "if constant_cols:\n",
    "    print(f\"Constant columns detected and dropped: {constant_cols}\")\n",
    "    df.drop(columns=constant_cols, inplace=True)\n",
    "    print(f\"New shape after dropping: {df.shape}\")\n",
    "else:\n",
    "    print(\"No constant columns detected.\")\n",
    "\n",
    "# Confirm target variable distribution\n",
    "print(\"\\nClass balance in 'Attrition':\")\n",
    "display(df['Attrition'].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe07bf8",
   "metadata": {},
   "source": [
    "## 4. Feature engineering pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af813a",
   "metadata": {},
   "source": [
    "### Feature engineering transformer\n",
    "\n",
    "This step defines a custom `FeatureEngineer` class that inherits from `BaseEstimator` and `TransformerMixin`, allowing integration into scikit-learn pipelines. The `transform()` method generates several new features based on domain logic.\n",
    "\n",
    "The operations performed include:\n",
    "\n",
    "Tenure-based features:\n",
    "- `TenureCategory`: Categorizes `YearsAtCompany` into four ordinal bins.\n",
    "- `YearsAtCompanyRatio`: Ratio of years at company to total working years.\n",
    "- `YearsInRoleRatio`: Ratio of years in current role to years at company.\n",
    "- `YearsWithManagerRatio`: Ratio of years with current manager to years at company.\n",
    "- `IncomePerYear`: Monthly income divided by years at company.\n",
    "\n",
    "Work history flags:\n",
    "- `ZeroCompanyTenureFlag`: Flags employees with 0 years at company.\n",
    "- `NoWorkHistoryFlag`: Flags employees with 0 years at company and 0 total working years.\n",
    "\n",
    "Job role grouping:\n",
    "- `JobRoleGroup`: Maps specific job titles into broader role categories such as technical and sales.\n",
    "\n",
    "Satisfaction features:\n",
    "- `SatisfactionAvg`: Average of environment, job, and relationship satisfaction.\n",
    "- `SatisfactionRange`: Difference between max and min satisfaction values.\n",
    "- `LowSatisfactionFlag`: Flags employees with any satisfaction metric less than or equal to 2.\n",
    "\n",
    "Interaction features:\n",
    "- `OverTime_JobLevel`: Combines overtime status and job level.\n",
    "- `Travel_Occupation`: Combines business travel and job role.\n",
    "\n",
    "Composite risk flag:\n",
    "- `StressRisk`: Flags employees who work overtime, have low satisfaction, and a satisfaction average below 2.5.\n",
    "\n",
    "The function returns the modified dataframe with new features added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b12052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "\n",
    "        df['TenureCategory'] = pd.cut(\n",
    "            df['YearsAtCompany'],\n",
    "            bins=[-1, 2, 5, 10, np.inf],\n",
    "            labels=['<3 yrs', '3–5 yrs', '6–10 yrs', '10+ yrs']\n",
    "        )\n",
    "\n",
    "        df['YearsAtCompanyRatio'] = df['YearsAtCompany'] / df['TotalWorkingYears'].replace(0, np.nan)\n",
    "        df['YearsInRoleRatio'] = df['YearsInCurrentRole'] / df['YearsAtCompany'].replace(0, np.nan)\n",
    "        df['YearsWithManagerRatio'] = df['YearsWithCurrManager'] / df['YearsAtCompany'].replace(0, np.nan)\n",
    "        df['IncomePerYear'] = df['MonthlyIncome'] / df['YearsAtCompany'].replace(0, np.nan)\n",
    "\n",
    "        df['ZeroCompanyTenureFlag'] = (df['YearsAtCompany'] == 0).astype(int)\n",
    "        df['NoWorkHistoryFlag'] = ((df['YearsAtCompany'] == 0) & (df['TotalWorkingYears'] == 0)).astype(int)\n",
    "\n",
    "        df['JobRoleGroup'] = df['JobRole'].replace({\n",
    "            'Laboratory Technician': 'Technical',\n",
    "            'Research Scientist': 'Technical',\n",
    "            'Healthcare Representative': 'Sales',\n",
    "            'Sales Executive': 'Sales',\n",
    "            'Sales Representative': 'Sales'\n",
    "        })\n",
    "\n",
    "        df['SatisfactionAvg'] = df[[\n",
    "            'EnvironmentSatisfaction', 'JobSatisfaction', 'RelationshipSatisfaction'\n",
    "        ]].mean(axis=1)\n",
    "\n",
    "        df['SatisfactionRange'] = (\n",
    "            df[['EnvironmentSatisfaction', 'JobSatisfaction', 'RelationshipSatisfaction']].max(axis=1) -\n",
    "            df[['EnvironmentSatisfaction', 'JobSatisfaction', 'RelationshipSatisfaction']].min(axis=1)\n",
    "        )\n",
    "\n",
    "        df['LowSatisfactionFlag'] = (\n",
    "            (df['EnvironmentSatisfaction'] <= 2) |\n",
    "            (df['JobSatisfaction'] <= 2) |\n",
    "            (df['RelationshipSatisfaction'] <= 2)\n",
    "        ).astype(int)\n",
    "\n",
    "        df['OverTime_JobLevel'] = df['OverTime'].astype(str) + \"_\" + df['JobLevel'].astype(str)\n",
    "        df['Travel_Occupation'] = df['BusinessTravel'].astype(str) + \"_\" + df['JobRole'].astype(str)\n",
    "\n",
    "        df['StressRisk'] = (\n",
    "            (df['OverTime'] == 'Yes') &\n",
    "            (df['LowSatisfactionFlag'] == 1) &\n",
    "            (df['SatisfactionAvg'] < 2.5)\n",
    "        ).astype(int)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de698b62",
   "metadata": {},
   "source": [
    "### ColumnTransformer: preprocessing steps\n",
    "\n",
    "This step defines a preprocessing pipeline using `ColumnTransformer` to handle different types of features appropriately:\n",
    "\n",
    "- `ordinal_col`: Encodes the `TenureCategory` feature with an explicit order: less than 3 years, 3–5 years, 6–10 years, and more than 10 years.\n",
    "- `nominal_cols`: Specifies all nominal (unordered categorical) features to be one-hot encoded.\n",
    "- `scale_cols`: Lists all continuous numerical features that should be standardized using `StandardScaler`.\n",
    "\n",
    "The `ColumnTransformer` applies:\n",
    "- `OrdinalEncoder` to ordinal features using the defined category order.\n",
    "- `OneHotEncoder` to nominal features, dropping the first category to avoid multicollinearity and ignoring unknown categories.\n",
    "- `StandardScaler` to continuous features to normalize them to zero mean and unit variance.\n",
    "\n",
    "All unspecified columns (binary flags) are passed through unchanged using `remainder='passthrough'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ordinal_col = ['TenureCategory']\n",
    "ordinal_map = [['<3 yrs', '3–5 yrs', '6–10 yrs', '10+ yrs']]\n",
    "\n",
    "nominal_cols = [\n",
    "    'BusinessTravel', 'Department', 'EducationField', 'JobRole',\n",
    "    'MaritalStatus', 'JobRoleGroup', 'Travel_Occupation', 'OverTime_JobLevel'\n",
    "]\n",
    "\n",
    "# Manually chosen continuous variables to scale\n",
    "scale_cols = [\n",
    "    'Age', 'DistanceFromHome', 'MonthlyIncome', 'YearsAtCompany',\n",
    "    'YearsInCurrentRole', 'YearsWithCurrManager', 'YearsSinceLastPromotion',\n",
    "    'TotalWorkingYears', 'YearsInRoleRatio', 'YearsWithManagerRatio',\n",
    "    'YearsAtCompanyRatio', 'IncomePerYear', 'SatisfactionAvg', 'SatisfactionRange'\n",
    "]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('ordinal', OrdinalEncoder(categories=ordinal_map), ordinal_col),\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), nominal_cols),\n",
    "    ('scale', StandardScaler(), scale_cols)\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87842a27",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline\n",
    "\n",
    "This pipeline combines all preprocessing steps into a single object using `sklearn.pipeline.Pipeline`.\n",
    "\n",
    "It consists of two stages:\n",
    "1. `features`: Applies the custom `FeatureEngineer` transformer, which performs domain-specific feature engineering such as generating tenure ratios, satisfaction flags, and interaction features.\n",
    "2. `preprocessing`: Applies the `ColumnTransformer` defined earlier, which encodes categorical variables and scales numeric features.\n",
    "\n",
    "This modular structure ensures all transformations are applied consistently and reproducibly, enabling clean integration with downstream modeling and tuning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbae13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('features', FeatureEngineer()),\n",
    "    ('preprocessing', preprocessor)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc81518e",
   "metadata": {},
   "source": [
    "### Exporting the preprocessing pipeline\n",
    "\n",
    "The complete preprocessing pipeline is serialized using `joblib` and saved to disk as a `.pkl` file.\n",
    "\n",
    "This allows the exact same transformations to be reused in other notebooks (such as `modeling.ipynb`) without recomputing feature engineering, encoding, or scaling steps. This is critical for maintaining consistency between training and testing environments.\n",
    "\n",
    "Saved to: `../models/preprocessing_pipeline.pkl`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c171419a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pipeline saved.\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(preprocessing_pipeline, '../models/preprocessing_pipeline.pkl')\n",
    "print(\"Preprocessing pipeline saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2680ef",
   "metadata": {},
   "source": [
    "# Preprocessing summary and next steps \n",
    "\n",
    "This notebook implemented the full preprocessing pipeline for the attrition dataset, including:\n",
    "\n",
    "- Custom feature engineering via a `FeatureEngineer` transformer\n",
    "- Encoding of ordinal and nominal categorical features using `OrdinalEncoder` and `OneHotEncoder`\n",
    "- Standardization of selected continuous variables using `StandardScaler`\n",
    "- Integration of all steps into a scikit-learn `Pipeline` for modular reuse\n",
    "- Exporting the complete pipeline to `../models/preprocessing_pipeline.pkl` using `joblib`\n",
    "\n",
    "The exported pipeline preserves all transformations and ensures consistent preprocessing during model training, evaluation, and deployment. It can be loaded into `modeling.ipynb` to prevent data leakage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm-attrition-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
