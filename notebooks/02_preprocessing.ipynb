{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b5cf274",
   "metadata": {},
   "source": [
    "# Cleaning and Preprocessing\n",
    "\n",
    "## 1. Notebook overview\n",
    "\n",
    "This notebook prepares the dataset for modeling by performing data cleaning, feature engineering, and preprocessing.\n",
    "\n",
    "Specifically, it:\n",
    "- Reloads the cleaned dataset from the previous EDA step.\n",
    "- Applies cleaning and formatting for consistency.\n",
    "- Encodes categorical variables appropriately.\n",
    "- Scales numerical features if needed.\n",
    "- Handles class imbalance in the target variable.\n",
    "- Splits the data into training and test sets.\n",
    "- Exports the preprocessed dataset for modeling.\n",
    "\n",
    "This is the second step in the pipeline following `01_eda.ipynb`, and it feeds into `03_modeling.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e44762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing environment initialized.\n"
     ]
    }
   ],
   "source": [
    "# Imports for preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Set options for display\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Preprocessing environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6e0ee",
   "metadata": {},
   "source": [
    "## 2. Reload data and confirm schema\n",
    "\n",
    "We load the cleaned dataset exported from the EDA notebook (`data_01.csv`) and verify its structure before proceeding with preprocessing.\n",
    "\n",
    "This step ensures:\n",
    "- The dataset was saved correctly.\n",
    "- The schema matches expectations (column names, data types).\n",
    "- There are no unexpected missing values or type mismatches introduced during export.\n",
    "\n",
    "We also validate that the dataset includes exactly the expected columns — no more, no less. This prevents issues downstream if column names are altered, dropped, or duplicated.\n",
    "\n",
    "The expected schema includes only meaningful, cleaned features after removing non-informative columns in the EDA phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2015a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column schema validation passed.\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_csv('../data/processed/data_01.csv')\n",
    "\n",
    "# Define expected column names after EDA cleanup\n",
    "expected_columns = [\n",
    "    'Age', 'Attrition', 'BusinessTravel', 'DailyRate', 'Department',\n",
    "    'DistanceFromHome', 'Education', 'EducationField', 'EnvironmentSatisfaction',\n",
    "    'Gender', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole',\n",
    "    'JobSatisfaction', 'MaritalStatus', 'MonthlyIncome', 'MonthlyRate',\n",
    "    'NumCompaniesWorked', 'OverTime', 'PercentSalaryHike', 'PerformanceRating',\n",
    "    'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears',\n",
    "    'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany',\n",
    "    'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'\n",
    "]\n",
    "\n",
    "# Get actual columns from the loaded DataFrame\n",
    "actual_columns = list(df.columns)\n",
    "\n",
    "# Compare against expected\n",
    "missing_columns = set(expected_columns) - set(actual_columns)\n",
    "unexpected_columns = set(actual_columns) - set(expected_columns)\n",
    "\n",
    "# Display results\n",
    "if not missing_columns and not unexpected_columns:\n",
    "    print(\"Column schema validation passed.\")\n",
    "else:\n",
    "    if missing_columns:\n",
    "        print(\"Missing columns:\", missing_columns)\n",
    "    if unexpected_columns:\n",
    "        print(\"Unexpected columns:\", unexpected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e977d",
   "metadata": {},
   "source": [
    "## 3. Cleaning checks\n",
    "\n",
    "We perform additional cleaning checks on the dataset before continuing preprocessing.\n",
    "\n",
    "This includes:\n",
    "- Verifying data types are appropriate.\n",
    "- Checking for unexpected nulls (none should exist).\n",
    "- Ensuring no constant or identifier columns remain.\n",
    "- Confirming target class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "279be89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No null values found.\n",
      "\n",
      "Data types:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Age                          int64\n",
       "Attrition                   object\n",
       "BusinessTravel              object\n",
       "DailyRate                    int64\n",
       "Department                  object\n",
       "DistanceFromHome             int64\n",
       "Education                    int64\n",
       "EducationField              object\n",
       "EnvironmentSatisfaction      int64\n",
       "Gender                      object\n",
       "HourlyRate                   int64\n",
       "JobInvolvement               int64\n",
       "JobLevel                     int64\n",
       "JobRole                     object\n",
       "JobSatisfaction              int64\n",
       "MaritalStatus               object\n",
       "MonthlyIncome                int64\n",
       "MonthlyRate                  int64\n",
       "NumCompaniesWorked           int64\n",
       "OverTime                    object\n",
       "PercentSalaryHike            int64\n",
       "PerformanceRating            int64\n",
       "RelationshipSatisfaction     int64\n",
       "StockOptionLevel             int64\n",
       "TotalWorkingYears            int64\n",
       "TrainingTimesLastYear        int64\n",
       "WorkLifeBalance              int64\n",
       "YearsAtCompany               int64\n",
       "YearsInCurrentRole           int64\n",
       "YearsSinceLastPromotion      int64\n",
       "YearsWithCurrManager         int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No constant columns detected.\n",
      "\n",
      "Class balance in 'Attrition':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Attrition\n",
       "No     0.839\n",
       "Yes    0.161\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for nulls (none expected)\n",
    "null_counts = df.isnull().sum()\n",
    "if null_counts.any():\n",
    "    print(\"Unexpected null values found:\")\n",
    "    display(null_counts[null_counts > 0])\n",
    "else:\n",
    "    print(\"No null values found.\")\n",
    "\n",
    "# Recheck data types\n",
    "print(\"\\nData types:\")\n",
    "display(df.dtypes)\n",
    "\n",
    "# Identify constant columns (only one unique value)\n",
    "nunique = df.nunique()\n",
    "constant_cols = nunique[nunique == 1].index.tolist()\n",
    "\n",
    "if constant_cols:\n",
    "    print(f\"Constant columns detected and dropped: {constant_cols}\")\n",
    "    df.drop(columns=constant_cols, inplace=True)\n",
    "    print(f\"New shape after dropping: {df.shape}\")\n",
    "else:\n",
    "    print(\"No constant columns detected.\")\n",
    "\n",
    "# Confirm target variable distribution\n",
    "print(\"\\nClass balance in 'Attrition':\")\n",
    "display(df['Attrition'].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a7660a",
   "metadata": {},
   "source": [
    "## 4. Feature engineering\n",
    "\n",
    "In this section, we enhance the feature space with domain-informed transformations and interactions that may improve model performance.\n",
    "\n",
    "We include:\n",
    "\n",
    "### Tenure-related features\n",
    "- `TenureCategory`: Bins `YearsAtCompany` into ranges to capture non-linear tenure patterns.\n",
    "- `YearsAtCompanyRatio`: Fraction of career spent at current company.\n",
    "- `YearsInRoleRatio`: Stability in the same role.\n",
    "- `YearsWithManagerRatio`: Indicates long-term relationship with supervisor.\n",
    "\n",
    "### Income-related features\n",
    "- `IncomePerYear`: Ratio of monthly income to tenure.\n",
    "- `HighIncomeShortTenure`: Flags early high earners (possible burnout risk).\n",
    "- `HighIncomeFlag`: Binary flag for income above median.\n",
    "- `HourlyRateBucket`: Bucketized hourly pay to reduce skew.\n",
    "- `IncomePerEduLevel`: Normalized income by education level.\n",
    "- `IncomeZScoreWithinEdu`: Z-score of income within each education level group.\n",
    "\n",
    "### Job role and travel groupings\n",
    "- `JobRoleGroup`: Simplifies rare job roles into broader categories.\n",
    "- `Travel_Occupation`: Interaction between business travel frequency and job role.\n",
    "\n",
    "### Satisfaction-based signals\n",
    "- `SatisfactionAvg`: Mean satisfaction across 3 dimensions.\n",
    "- `SatisfactionRange`: Range between highest and lowest satisfaction.\n",
    "- `LowSatisfactionFlag`: Any satisfaction dimension rated ≤ 2.\n",
    "\n",
    "### Interaction and composite flags\n",
    "- `OverTime_JobLevel`: Combines overtime flag with job level.\n",
    "- `StressRisk`: Composite risk index based on overtime, commute, and satisfaction.\n",
    "\n",
    "These features are designed to expose nonlinear effects, interactions, and latent variables the model might otherwise miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac14cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All engineered features added.\n"
     ]
    }
   ],
   "source": [
    "# Tenure-based Features\n",
    "df['TenureCategory'] = pd.cut(\n",
    "    df['YearsAtCompany'],\n",
    "    bins=[-1, 2, 5, 10, np.inf],\n",
    "    labels=['<3 yrs', '3-5 yrs', '6-10 yrs', '10+ yrs']\n",
    ")\n",
    "\n",
    "df['YearsAtCompanyRatio'] = df['YearsAtCompany'] / df['TotalWorkingYears'].replace(0, np.nan) # Avoids division by 0 - nulls created here are handled in the next cell\n",
    "df['YearsInRoleRatio'] = df['YearsInCurrentRole'] / df['YearsAtCompany'].replace(0, np.nan)\n",
    "df['YearsWithManagerRatio'] = df['YearsWithCurrManager'] / df['YearsAtCompany'].replace(0, np.nan)\n",
    "\n",
    "# Income-based Features\n",
    "df['IncomePerYear'] = df['MonthlyIncome'] / df['YearsAtCompany'].replace(0, np.nan)\n",
    "df['HighIncomeShortTenure'] = (\n",
    "    (df['MonthlyIncome'] > df['MonthlyIncome'].median()) &\n",
    "    (df['YearsAtCompany'] < 3)\n",
    ").astype(int)\n",
    "df['HighIncomeFlag'] = (df['MonthlyIncome'] > df['MonthlyIncome'].median()).astype(int)\n",
    "\n",
    "df['HourlyRateBucket'] = pd.qcut(df['HourlyRate'], q=4, labels=False)\n",
    "df['IncomePerEduLevel'] = df['MonthlyIncome'] / df['Education'].replace(0, np.nan)\n",
    "\n",
    "df['IncomeZScoreWithinEdu'] = df.groupby('Education')['MonthlyIncome'].transform(\n",
    "    lambda x: (x - x.mean()) / x.std(ddof=0) # Population standard deviation; population = all employees at each education level; avoids dividing by 0\n",
    ")\n",
    "\n",
    "# Job Role Groupings\n",
    "df['JobRoleGroup'] = df['JobRole'].replace({\n",
    "    'Laboratory Technician': 'Technical',\n",
    "    'Research Scientist': 'Technical',\n",
    "    'Healthcare Representative': 'Sales',\n",
    "    'Sales Executive': 'Sales',\n",
    "    'Sales Representative': 'Sales'\n",
    "})\n",
    "\n",
    "# Satisfaction Features\n",
    "df['SatisfactionAvg'] = df[[\n",
    "    'EnvironmentSatisfaction', 'JobSatisfaction', 'RelationshipSatisfaction'\n",
    "]].mean(axis=1)\n",
    "\n",
    "df['SatisfactionRange'] = df[[\n",
    "    'EnvironmentSatisfaction', 'JobSatisfaction', 'RelationshipSatisfaction'\n",
    "]].max(axis=1) - df[[\n",
    "    'EnvironmentSatisfaction', 'JobSatisfaction', 'RelationshipSatisfaction'\n",
    "]].min(axis=1)\n",
    "\n",
    "df['LowSatisfactionFlag'] = (\n",
    "    (df['EnvironmentSatisfaction'] <= 2) |\n",
    "    (df['JobSatisfaction'] <= 2) |\n",
    "    (df['RelationshipSatisfaction'] <= 2)\n",
    ").astype(int)\n",
    "\n",
    "# Interaction Features\n",
    "df['OverTime_JobLevel'] = df['OverTime'].astype(str) + \"_\" + df['JobLevel'].astype(str)\n",
    "df['Travel_Occupation'] = df['BusinessTravel'].astype(str) + \"_\" + df['JobRole'].astype(str)\n",
    "\n",
    "# Composite Risk Flag\n",
    "df['StressRisk'] = (\n",
    "    (df['OverTime'] == 'Yes') &\n",
    "    (df['DistanceFromHome'] > 10) &\n",
    "    (df['SatisfactionAvg'] < 2.5)\n",
    ").astype(int)\n",
    "\n",
    "print(\"All engineered features added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd7cd89",
   "metadata": {},
   "source": [
    "#### Handle division-based nulls and flag zero-tenure employees\n",
    "\n",
    "Several ratio features (`YearsInRoleRatio`, `YearsWithManagerRatio`, `IncomePerYear`, `YearsAtCompanyRatio`) use `YearsAtCompany` as a denominator. For a small number of employees where `YearsAtCompany == 0`, these ratios became `NaN` due to division by zero.\n",
    "\n",
    "To preserve eliminate these nulls:\n",
    "\n",
    "- All affected ratio columns are imputed with `0`, reflecting minimal tenure context (new hires, etc).\n",
    "- A `ZeroCompanyTenureFlag` marks these edge cases for interpretability.\n",
    "- A `NoWorkHistoryFlag` separately flags employees with no prior work experience (`TotalWorkingYears == 0`).\n",
    "\n",
    "This ensures the dataset remains numerically stable while retaining signal from meaningful outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4919fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YearsAtCompanyRatio\n",
       "1.000000    463\n",
       "0.500000     59\n",
       "0.833333     55\n",
       "0.000000     44\n",
       "0.800000     41\n",
       "           ... \n",
       "0.863636      1\n",
       "0.722222      1\n",
       "0.034483      1\n",
       "0.680000      1\n",
       "0.529412      1\n",
       "Name: count, Length: 192, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill ratio-based nulls caused by YearsAtCompany == 0\n",
    "df['YearsInRoleRatio'] = df['YearsInRoleRatio'].fillna(0)\n",
    "df['YearsWithManagerRatio'] = df['YearsWithManagerRatio'].fillna(0)\n",
    "df['IncomePerYear'] = df['IncomePerYear'].fillna(0)\n",
    "df['YearsAtCompanyRatio'] = df['YearsAtCompanyRatio'].fillna(0)\n",
    "\n",
    "# Flag structural edge cases for model interpretability\n",
    "df['ZeroCompanyTenureFlag'] = (df['YearsAtCompany'] == 0).astype(int)\n",
    "df['NoWorkHistoryFlag'] = ((df['YearsAtCompany'] == 0) & (df['TotalWorkingYears'] == 0)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30218050",
   "metadata": {},
   "source": [
    "## 5. Encoding categorical variables\n",
    "\n",
    "To prepare the dataset for modeling, we encode categorical variables into numerical form.\n",
    "\n",
    "We apply:\n",
    "- **Label encoding** for binary and ordinal variables (`Attrition`, `TenureCategory`, ...)\n",
    "- **One-hot encoding** for nominal variables with more than 2 categories\n",
    "\n",
    "This transformation ensures that all features are numeric, as required by logistic regression. We also drop one dummy column per one-hot-encoded feature to avoid multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ad8c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1470 entries, 0 to 1469\n",
      "Data columns (total 50 columns):\n",
      " #   Column                    Non-Null Count  Dtype   \n",
      "---  ------                    --------------  -----   \n",
      " 0   Age                       1470 non-null   int64   \n",
      " 1   Attrition                 1470 non-null   object  \n",
      " 2   BusinessTravel            1470 non-null   object  \n",
      " 3   DailyRate                 1470 non-null   int64   \n",
      " 4   Department                1470 non-null   object  \n",
      " 5   DistanceFromHome          1470 non-null   int64   \n",
      " 6   Education                 1470 non-null   int64   \n",
      " 7   EducationField            1470 non-null   object  \n",
      " 8   EnvironmentSatisfaction   1470 non-null   int64   \n",
      " 9   Gender                    1470 non-null   object  \n",
      " 10  HourlyRate                1470 non-null   int64   \n",
      " 11  JobInvolvement            1470 non-null   int64   \n",
      " 12  JobLevel                  1470 non-null   int64   \n",
      " 13  JobRole                   1470 non-null   object  \n",
      " 14  JobSatisfaction           1470 non-null   int64   \n",
      " 15  MaritalStatus             1470 non-null   object  \n",
      " 16  MonthlyIncome             1470 non-null   int64   \n",
      " 17  MonthlyRate               1470 non-null   int64   \n",
      " 18  NumCompaniesWorked        1470 non-null   int64   \n",
      " 19  OverTime                  1470 non-null   object  \n",
      " 20  PercentSalaryHike         1470 non-null   int64   \n",
      " 21  PerformanceRating         1470 non-null   int64   \n",
      " 22  RelationshipSatisfaction  1470 non-null   int64   \n",
      " 23  StockOptionLevel          1470 non-null   int64   \n",
      " 24  TotalWorkingYears         1470 non-null   int64   \n",
      " 25  TrainingTimesLastYear     1470 non-null   int64   \n",
      " 26  WorkLifeBalance           1470 non-null   int64   \n",
      " 27  YearsAtCompany            1470 non-null   int64   \n",
      " 28  YearsInCurrentRole        1470 non-null   int64   \n",
      " 29  YearsSinceLastPromotion   1470 non-null   int64   \n",
      " 30  YearsWithCurrManager      1470 non-null   int64   \n",
      " 31  TenureCategory            1470 non-null   category\n",
      " 32  YearsAtCompanyRatio       1470 non-null   float64 \n",
      " 33  YearsInRoleRatio          1470 non-null   float64 \n",
      " 34  YearsWithManagerRatio     1470 non-null   float64 \n",
      " 35  IncomePerYear             1470 non-null   float64 \n",
      " 36  HighIncomeShortTenure     1470 non-null   int64   \n",
      " 37  HighIncomeFlag            1470 non-null   int64   \n",
      " 38  HourlyRateBucket          1470 non-null   int64   \n",
      " 39  IncomePerEduLevel         1470 non-null   float64 \n",
      " 40  IncomeZScoreWithinEdu     1470 non-null   float64 \n",
      " 41  JobRoleGroup              1470 non-null   object  \n",
      " 42  SatisfactionAvg           1470 non-null   float64 \n",
      " 43  SatisfactionRange         1470 non-null   int64   \n",
      " 44  LowSatisfactionFlag       1470 non-null   int64   \n",
      " 45  OverTime_JobLevel         1470 non-null   object  \n",
      " 46  Travel_Occupation         1470 non-null   object  \n",
      " 47  StressRisk                1470 non-null   int64   \n",
      " 48  ZeroCompanyTenureFlag     1470 non-null   int64   \n",
      " 49  NoWorkHistoryFlag         1470 non-null   int64   \n",
      "dtypes: category(1), float64(7), int64(31), object(11)\n",
      "memory usage: 564.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25824cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables encoded.\n",
      "Encoded shape: (1470, 101)\n",
      "\n",
      "🆕 59 new columns added via one-hot encoding:\n",
      "\n",
      "  BusinessTravel_Travel_Frequently\n",
      "  BusinessTravel_Travel_Rarely\n",
      "  Department_Research & Development\n",
      "  Department_Sales\n",
      "  EducationField_Life Sciences\n",
      "  EducationField_Marketing\n",
      "  EducationField_Medical\n",
      "  EducationField_Other\n",
      "  EducationField_Technical Degree\n",
      "  JobRoleGroup_Manager\n",
      "  JobRoleGroup_Manufacturing Director\n",
      "  JobRoleGroup_Research Director\n",
      "  JobRoleGroup_Sales\n",
      "  JobRoleGroup_Technical\n",
      "  JobRole_Human Resources\n",
      "  JobRole_Laboratory Technician\n",
      "  JobRole_Manager\n",
      "  JobRole_Manufacturing Director\n",
      "  JobRole_Research Director\n",
      "  JobRole_Research Scientist\n",
      "  JobRole_Sales Executive\n",
      "  JobRole_Sales Representative\n",
      "  MaritalStatus_Married\n",
      "  MaritalStatus_Single\n",
      "  OverTime_JobLevel_No_2\n",
      "  OverTime_JobLevel_No_3\n",
      "  OverTime_JobLevel_No_4\n",
      "  OverTime_JobLevel_No_5\n",
      "  OverTime_JobLevel_Yes_1\n",
      "  OverTime_JobLevel_Yes_2\n",
      "  OverTime_JobLevel_Yes_3\n",
      "  OverTime_JobLevel_Yes_4\n",
      "  OverTime_JobLevel_Yes_5\n",
      "  Travel_Occupation_Non-Travel_Human Resources\n",
      "  Travel_Occupation_Non-Travel_Laboratory Technician\n",
      "  Travel_Occupation_Non-Travel_Manager\n",
      "  Travel_Occupation_Non-Travel_Manufacturing Director\n",
      "  Travel_Occupation_Non-Travel_Research Director\n",
      "  Travel_Occupation_Non-Travel_Research Scientist\n",
      "  Travel_Occupation_Non-Travel_Sales Executive\n",
      "  Travel_Occupation_Non-Travel_Sales Representative\n",
      "  Travel_Occupation_Travel_Frequently_Healthcare Representative\n",
      "  Travel_Occupation_Travel_Frequently_Human Resources\n",
      "  Travel_Occupation_Travel_Frequently_Laboratory Technician\n",
      "  Travel_Occupation_Travel_Frequently_Manager\n",
      "  Travel_Occupation_Travel_Frequently_Manufacturing Director\n",
      "  Travel_Occupation_Travel_Frequently_Research Director\n",
      "  Travel_Occupation_Travel_Frequently_Research Scientist\n",
      "  Travel_Occupation_Travel_Frequently_Sales Executive\n",
      "  Travel_Occupation_Travel_Frequently_Sales Representative\n",
      "  Travel_Occupation_Travel_Rarely_Healthcare Representative\n",
      "  Travel_Occupation_Travel_Rarely_Human Resources\n",
      "  Travel_Occupation_Travel_Rarely_Laboratory Technician\n",
      "  Travel_Occupation_Travel_Rarely_Manager\n",
      "  Travel_Occupation_Travel_Rarely_Manufacturing Director\n",
      "  Travel_Occupation_Travel_Rarely_Research Director\n",
      "  Travel_Occupation_Travel_Rarely_Research Scientist\n",
      "  Travel_Occupation_Travel_Rarely_Sales Executive\n",
      "  Travel_Occupation_Travel_Rarely_Sales Representative\n",
      "\n",
      "BusinessTravel → 2 dummy columns:\n",
      "  BusinessTravel_Travel_Frequently\n",
      "  BusinessTravel_Travel_Rarely\n",
      "\n",
      "Department → 2 dummy columns:\n",
      "  Department_Research & Development\n",
      "  Department_Sales\n",
      "\n",
      "EducationField → 5 dummy columns:\n",
      "  EducationField_Life Sciences\n",
      "  EducationField_Marketing\n",
      "  EducationField_Medical\n",
      "  EducationField_Other\n",
      "  EducationField_Technical Degree\n",
      "\n",
      "JobRole → 8 dummy columns:\n",
      "  JobRole_Human Resources\n",
      "  JobRole_Laboratory Technician\n",
      "  JobRole_Manager\n",
      "  JobRole_Manufacturing Director\n",
      "  JobRole_Research Director\n",
      "  JobRole_Research Scientist\n",
      "  JobRole_Sales Executive\n",
      "  JobRole_Sales Representative\n",
      "\n",
      "JobRoleGroup → 5 dummy columns:\n",
      "  JobRoleGroup_Manager\n",
      "  JobRoleGroup_Manufacturing Director\n",
      "  JobRoleGroup_Research Director\n",
      "  JobRoleGroup_Sales\n",
      "  JobRoleGroup_Technical\n",
      "\n",
      "MaritalStatus → 2 dummy columns:\n",
      "  MaritalStatus_Married\n",
      "  MaritalStatus_Single\n",
      "\n",
      "OverTime_JobLevel → 9 dummy columns:\n",
      "  OverTime_JobLevel_No_2\n",
      "  OverTime_JobLevel_No_3\n",
      "  OverTime_JobLevel_No_4\n",
      "  OverTime_JobLevel_No_5\n",
      "  OverTime_JobLevel_Yes_1\n",
      "  OverTime_JobLevel_Yes_2\n",
      "  OverTime_JobLevel_Yes_3\n",
      "  OverTime_JobLevel_Yes_4\n",
      "  OverTime_JobLevel_Yes_5\n",
      "\n",
      "Travel_Occupation → 26 dummy columns:\n",
      "  Travel_Occupation_Non-Travel_Human Resources\n",
      "  Travel_Occupation_Non-Travel_Laboratory Technician\n",
      "  Travel_Occupation_Non-Travel_Manager\n",
      "  Travel_Occupation_Non-Travel_Manufacturing Director\n",
      "  Travel_Occupation_Non-Travel_Research Director\n",
      "  Travel_Occupation_Non-Travel_Research Scientist\n",
      "  Travel_Occupation_Non-Travel_Sales Executive\n",
      "  Travel_Occupation_Non-Travel_Sales Representative\n",
      "  Travel_Occupation_Travel_Frequently_Healthcare Representative\n",
      "  Travel_Occupation_Travel_Frequently_Human Resources\n",
      "  Travel_Occupation_Travel_Frequently_Laboratory Technician\n",
      "  Travel_Occupation_Travel_Frequently_Manager\n",
      "  Travel_Occupation_Travel_Frequently_Manufacturing Director\n",
      "  Travel_Occupation_Travel_Frequently_Research Director\n",
      "  Travel_Occupation_Travel_Frequently_Research Scientist\n",
      "  Travel_Occupation_Travel_Frequently_Sales Executive\n",
      "  Travel_Occupation_Travel_Frequently_Sales Representative\n",
      "  Travel_Occupation_Travel_Rarely_Healthcare Representative\n",
      "  Travel_Occupation_Travel_Rarely_Human Resources\n",
      "  Travel_Occupation_Travel_Rarely_Laboratory Technician\n",
      "  Travel_Occupation_Travel_Rarely_Manager\n",
      "  Travel_Occupation_Travel_Rarely_Manufacturing Director\n",
      "  Travel_Occupation_Travel_Rarely_Research Director\n",
      "  Travel_Occupation_Travel_Rarely_Research Scientist\n",
      "  Travel_Occupation_Travel_Rarely_Sales Executive\n",
      "  Travel_Occupation_Travel_Rarely_Sales Representative\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "\n",
    "# Make a copy to work from\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Binary Label Encoding\n",
    "binary_cols = [\n",
    "    'Attrition', 'OverTime', 'Gender', 'HighIncomeFlag',\n",
    "    'LowSatisfactionFlag', 'HighIncomeShortTenure', 'StressRisk'\n",
    "]\n",
    "\n",
    "for col in binary_cols:\n",
    "    df_encoded[col] = LabelEncoder().fit_transform(df_encoded[col])\n",
    "\n",
    "# Ordinal Encoding\n",
    "df_encoded['TenureCategory'] = df_encoded['TenureCategory'].map({\n",
    "    '<3 yrs': 0,\n",
    "    '3-5 yrs': 1,\n",
    "    '6-10 yrs': 2,\n",
    "    '10+ yrs': 3\n",
    "})\n",
    "\n",
    "# One-hot Encoding\n",
    "nominal_cols = [\n",
    "    'BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus',\n",
    "    'JobRoleGroup', 'Travel_Occupation', 'OverTime_JobLevel'\n",
    "]\n",
    "\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=nominal_cols, drop_first=True) # Avoid multicollinearity\n",
    "\n",
    "print(\"Categorical variables encoded.\")\n",
    "print(f\"Encoded shape: {df_encoded.shape}\")\n",
    "\n",
    "# Inspect new columns\n",
    "original_cols = set(df.columns)\n",
    "encoded_cols = set(df_encoded.columns)\n",
    "new_columns = sorted(encoded_cols - original_cols)\n",
    "\n",
    "if new_columns:\n",
    "    print(f\"\\n{len(new_columns)} new columns added via one-hot encoding:\\n\")\n",
    "    for col in new_columns:\n",
    "        print(f\"  {col}\")\n",
    "else:\n",
    "    print(\"No new columns were added via one-hot encoding.\")\n",
    "\n",
    "# Group one-hot columns by source\n",
    "grouped_dummies = defaultdict(list)\n",
    "for col in new_columns:\n",
    "    for original in nominal_cols:\n",
    "        if col.startswith(original + \"_\"):\n",
    "            grouped_dummies[original].append(col)\n",
    "\n",
    "# Display grouped dummy columns\n",
    "for key in sorted(grouped_dummies):\n",
    "    values = grouped_dummies[key]\n",
    "    print(f\"\\n{key} → {len(values)} dummy columns:\")\n",
    "    for v in sorted(values):\n",
    "        print(f\"  {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c880a",
   "metadata": {},
   "source": [
    "### Removing rare dummy variables\n",
    "\n",
    "Some one-hot encoded columns may represent categories with very few samples, which can introduce instability during training. We drop dummy variables that appear in fewer than 10 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "600bce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 3 rare dummy columns (fewer than 10 observations):\n",
      "  Travel_Occupation_Non-Travel_Human Resources\n",
      "  Travel_Occupation_Non-Travel_Research Director\n",
      "  Travel_Occupation_Non-Travel_Sales Representative\n"
     ]
    }
   ],
   "source": [
    "# Define minimum threshold\n",
    "min_count = 10\n",
    "\n",
    "# Identify dummy columns that were just created\n",
    "original_cols = set(df.columns)\n",
    "encoded_cols = set(df_encoded.columns)\n",
    "new_dummies = sorted(encoded_cols - original_cols)\n",
    "\n",
    "# Check frequency of each dummy column\n",
    "rare_dummies = [col for col in new_dummies if df_encoded[col].sum() < min_count]\n",
    "\n",
    "# Drop and report\n",
    "if rare_dummies:\n",
    "    df_encoded.drop(columns=rare_dummies, inplace=True)\n",
    "    print(f\"Dropped {len(rare_dummies)} rare dummy columns (fewer than {min_count} observations):\")\n",
    "    for col in rare_dummies:\n",
    "        print(f\"  {col}\")\n",
    "else:\n",
    "    print(\"No rare dummy columns to drop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb6636d",
   "metadata": {},
   "source": [
    "## 6. Feature scaling\n",
    "\n",
    "Logistic regression is sensitive to the scale of input features.\n",
    "\n",
    "In this step, we:\n",
    "- Identify numerical features that are not binary or categorical\n",
    "- Apply `StandardScaler` to normalize those features\n",
    "\n",
    "Note: Scaling is not strictly required for decision trees or ensembles, but we apply it for flexibility and consistency across modeling approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d7d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Identify columns to scale: numerical\n",
    "numeric_cols = df_encoded.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Detect binary columns (0 or 1 only)\n",
    "binary_cols = [col for col in df_encoded.columns\n",
    "               if df_encoded[col].dropna().nunique() == 2 and set(df_encoded[col].unique()) <= {0, 1}]\n",
    "\n",
    "# Exclude binary and other manually flagged columns\n",
    "exclude = set(binary_cols + [\n",
    "    'Attrition',  # target\n",
    "    'HighIncomeFlag', 'LowSatisfactionFlag',\n",
    "    'HighIncomeShortTenure', 'StressRisk'\n",
    "])\n",
    "\n",
    "# Keep only those that are continuous\n",
    "scale_cols = [col for col in numeric_cols if col not in exclude]\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply scaling\n",
    "df_encoded[scale_cols] = scaler.fit_transform(df_encoded[scale_cols])\n",
    "\n",
    "print(f\"Scaled {len(scale_cols)} numeric features.\")\n",
    "print(\"Scaled columns:\")\n",
    "for col in scale_cols:\n",
    "    print(f\"  {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0729af",
   "metadata": {},
   "source": [
    "### Histogram comparison: Before and after scaling\n",
    "\n",
    "We plot the distribution of selected numeric features before and after scaling. This helps verify that scaling transformed the features to standard normal (mean ≈ 0, std ≈ 1), while preserving their shape and distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Prepare original and scaled versions\n",
    "df_unscaled = df.copy()\n",
    "df_scaled = df_encoded.copy()\n",
    "\n",
    "# Set up grid\n",
    "num_features = len(scale_cols)\n",
    "cols_per_row = 2  # Original vs scaled\n",
    "rows = num_features\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols_per_row, figsize=(12, rows * 3))\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "for i, col in enumerate(scale_cols):\n",
    "    # Original\n",
    "    sns.histplot(df_unscaled[col], kde=True, ax=axes[i, 0], bins=30, color='skyblue')\n",
    "    axes[i, 0].set_title(f\"Original: {col}\")\n",
    "    \n",
    "    # Scaled\n",
    "    sns.histplot(df_scaled[col], kde=True, ax=axes[i, 1], bins=30, color='salmon')\n",
    "    axes[i, 1].set_title(f\"Scaled: {col}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Before vs After Scaling (All Numeric Features)\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a44eb",
   "metadata": {},
   "source": [
    "## 7. Train-test split\n",
    "\n",
    "We split the dataset into training and testing sets to evaluate generalization.\n",
    "\n",
    "Key considerations:\n",
    "- We use a stratified split on the target (`Attrition`) to preserve class balance.\n",
    "- 80% of the data is used for training; 20% is held out for testing.\n",
    "- Random seed is set for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define feature matrix and target\n",
    "X = df_encoded.drop(columns='Attrition')\n",
    "y = df_encoded['Attrition']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Confirm shape and class balance\n",
    "print(f\"Split complete:\")\n",
    "print(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"  X_test:  {X_test.shape}, y_test:  {y_test.shape}\")\n",
    "\n",
    "print(\"\\nClass distribution in y_train:\")\n",
    "display(y_train.value_counts(normalize=True).round(3))\n",
    "\n",
    "print(\"Class distribution in y_test:\")\n",
    "display(y_test.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d8c57",
   "metadata": {},
   "source": [
    "## 8. Handling class imbalance\n",
    "\n",
    "The target variable `Attrition` is imbalanced, with far more `No` than `Yes` cases.\n",
    "\n",
    "To address this:\n",
    "- We apply **SMOTE (Synthetic Minority Oversampling Technique)** to the training data.\n",
    "- SMOTE generates synthetic examples of the minority class by interpolating between existing ones.\n",
    "- We apply it **only to the training set** to avoid information leakage.\n",
    "\n",
    "This step is useful when using models sensitive to class imbalance (e.g., logistic regression, SVM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=RANDOM_STATE)\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Confirm result\n",
    "print(\"SMOTE applied to training data.\")\n",
    "print(f\"  X_train before: {X_train.shape}, after: {X_train_resampled.shape}\")\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "display(y_train_resampled.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e487b",
   "metadata": {},
   "source": [
    "## 9. Export preprocessed dataset\n",
    "\n",
    "We export the resampled and original datasets to the `../data/processed/` directory for use in the modeling phase.\n",
    "\n",
    "Saved files:\n",
    "- `X_train.csv`, `y_train.csv` — raw stratified training set\n",
    "- `X_test.csv`, `y_test.csv` — untouched test set\n",
    "- `X_train_resampled.csv`, `y_train_resampled.csv` — SMOTE-balanced training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b7e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"../data/processed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Export each component safely\n",
    "exports = {\n",
    "    \"X_train.csv\": X_train,\n",
    "    \"y_train.csv\": y_train,\n",
    "    \"X_test.csv\": X_test,\n",
    "    \"y_test.csv\": y_test,\n",
    "    \"X_train_resampled.csv\": X_train_resampled,\n",
    "    \"y_train_resampled.csv\": y_train_resampled\n",
    "}\n",
    "\n",
    "for filename, df_to_export in exports.items():\n",
    "    try:\n",
    "        full_path = os.path.join(output_dir, filename)\n",
    "        df_to_export.to_csv(full_path, index=False)\n",
    "        print(f\"✅ Exported: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to export {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60caf062",
   "metadata": {},
   "source": [
    "# Preprocessing Summary and Next Steps\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "- Reloaded and validated the cleaned dataset (`data_01.csv`)\n",
    "- Performed final data integrity checks (nulls, constants, schema)\n",
    "- Engineered domain-relevant features (e.g., tenure ratios, satisfaction flags)\n",
    "- Encoded categorical variables using label encoding and one-hot encoding\n",
    "- Scaled numeric features to standard normal\n",
    "- Applied a stratified train-test split to preserve class distribution\n",
    "- Handled class imbalance using SMOTE on the training set\n",
    "- Exported all relevant datasets for downstream modeling\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps (Modeling Phase)\n",
    "In the next notebook (`03_modeling.ipynb`), we will:\n",
    "\n",
    "- Load the preprocessed datasets\n",
    "- Train baseline classification models (Logistic Regression, Random Forest, etc.)\n",
    "- Evaluate using accuracy, recall, precision, F1-score, and ROC AUC\n",
    "- Tune hyperparameters and compare model performance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm-attrition-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
