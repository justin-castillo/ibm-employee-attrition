{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "614357b9",
   "metadata": {},
   "source": [
    "# Modeling: Predicting Employee Attrition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c9b71a",
   "metadata": {},
   "source": [
    "## 1. Notebook overview\n",
    "\n",
    "This notebook trains and evaluates classification models to predict employee attrition.\n",
    "\n",
    "Objectives:\n",
    "- Load preprocessed training and test sets\n",
    "- Train baseline models\n",
    "- Evaluate model performance using classification metrics\n",
    "- Tune hyperparameters for best-performing models\n",
    "- Save final model for interpretation in the next notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bbef7d",
   "metadata": {},
   "source": [
    "## 2. Load preprocessed datasets\n",
    "\n",
    "We load the training and test sets exported from the preprocessing notebook.\n",
    "- `X_train_resampled.csv` and `y_train_resampled.csv` (SMOTE-balanced training)\n",
    "- `X_test.csv` and `y_test.csv` (untouched test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d3392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define base directory\n",
    "base_path = \"../data/processed\"\n",
    "\n",
    "# Load CSVs\n",
    "X_train = pd.read_csv(f\"{base_path}/X_train.csv\")\n",
    "y_train = pd.read_csv(f\"{base_path}/y_train.csv\")\n",
    "\n",
    "X_test = pd.read_csv(f\"{base_path}/X_test.csv\")\n",
    "y_test = pd.read_csv(f\"{base_path}/y_test.csv\")\n",
    "\n",
    "X_train_resampled = pd.read_csv(f\"{base_path}/X_train_resampled.csv\")\n",
    "y_train_resampled = pd.read_csv(f\"{base_path}/y_train_resampled.csv\")\n",
    "\n",
    "# --- Shape confirmation ---\n",
    "print(\"Data loaded successfully:\")\n",
    "print(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"  X_test:  {X_test.shape}, y_test:  {y_test.shape}\")\n",
    "print(f\"  X_train_resampled: {X_train_resampled.shape}, y_train_resampled: {y_train_resampled.shape}\")\n",
    "\n",
    "# --- Null checks ---\n",
    "datasets = {\n",
    "    \"X_train\": X_train,\n",
    "    \"y_train\": y_train,\n",
    "    \"X_test\": X_test,\n",
    "    \"y_test\": y_test,\n",
    "    \"X_train_resampled\": X_train_resampled,\n",
    "    \"y_train_resampled\": y_train_resampled\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ” Null value check:\")\n",
    "for name, df in datasets.items():\n",
    "    nulls = df.isnull().sum().sum()\n",
    "    if nulls > 0:\n",
    "        print(f\"{name} contains {nulls} null values.\")\n",
    "    else:\n",
    "        print(f\"{name} has no nulls.\")\n",
    "\n",
    "# --- Column consistency check ---\n",
    "if list(X_train.columns) != list(X_test.columns) or list(X_train.columns) != list(X_train_resampled.columns):\n",
    "    print(\"\\nWarning: Column mismatch between training/test/resampled sets.\")\n",
    "else:\n",
    "    print(\"\\nAll feature sets have consistent columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb21266",
   "metadata": {},
   "source": [
    "## 3. Train baseline modeluntuned \n",
    "\n",
    "We train a set of baseline models to establish initial performance benchmarks.\n",
    "\n",
    "Models:\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- XGBoost or CatBoost (if included in environment)\n",
    "\n",
    "These models are evaluated using accuracy, recall, precision, F1-score, and ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_auc_score,\n",
    "    RocCurveDisplay\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize and train model\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_proba = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# --- Evaluation metrics ---\n",
    "print(\"ðŸ“Š Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(f\"\\nðŸ” ROC AUC: {roc_auc_score(y_test, y_proba):.3f}\")\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logreg.classes_)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
    "plt.show()\n",
    "\n",
    "# --- ROC Curve ---\n",
    "RocCurveDisplay.from_predictions(y_test, y_proba)\n",
    "plt.title(\"ROC Curve - Logistic Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3935d6b",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter tuning (Logistic Regression)\n",
    "\n",
    "We optimize the Logistic Regression model using `GridSearchCV` with stratified 5-fold cross-validation.\n",
    "\n",
    "### Tuned parameters:\n",
    "- `penalty`: Regularization type (`l1`, `l2`)\n",
    "- `C`: Inverse regularization strength (smaller = stronger penalty)\n",
    "- `class_weight`: Handles imbalance by weighting the minority class\n",
    "- `solver`: Optimization algorithm\n",
    "\n",
    "### Scoring:\n",
    "- Primary metric: **ROC AUC**\n",
    "- Cross-validation: **Stratified 5-fold**\n",
    "\n",
    "After tuning, we retrain the best model and evaluate it again on the untouched test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create logistic regression pipeline with scaler\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Redundant but ensures safe scaling inside CV folds\n",
    "    ('logreg', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'logreg__penalty': ['l1', 'l2'],\n",
    "    'logreg__C': np.logspace(-3, 1, 5),  # [0.001, 0.01, 0.1, 1, 10]\n",
    "    'logreg__solver': ['liblinear'],  # supports both l1 and l2\n",
    "    'logreg__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Initialize grid search\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit to resampled training data\n",
    "grid.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Best model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Results\n",
    "print(\"âœ… Grid Search complete.\")\n",
    "print(f\"Best ROC AUC: {grid.best_score_:.3f}\")\n",
    "print(\"Best parameters:\")\n",
    "for k, v in grid.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ce560",
   "metadata": {},
   "source": [
    "## Comparison: Untuned vs Tuned Logistic Regression\n",
    "\n",
    "We compare performance between the original (default) logistic regression model and the tuned model from GridSearchCV using the same evaluation metrics:\n",
    "\n",
    "- Classification report\n",
    "- ROC AUC\n",
    "- Confusion matrix\n",
    "- ROC curve\n",
    "\n",
    "This helps determine whether hyperparameter tuning produced a meaningful gain in predictive performance and class separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c38b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Untuned model prediction (logreg from earlier cell) ---\n",
    "y_pred_untuned = logreg.predict(X_test)\n",
    "y_proba_untuned = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# --- Tuned model prediction ---\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "y_proba_tuned = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# --- Metric comparison ---\n",
    "def summarize_metrics(name, y_true, y_pred, y_proba):\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * len(name))\n",
    "    print(f\"Accuracy:  {accuracy_score(y_true, y_pred):.3f}\")\n",
    "    print(f\"Precision: {precision_score(y_true, y_pred):.3f}\")\n",
    "    print(f\"Recall:    {recall_score(y_true, y_pred):.3f}\")\n",
    "    print(f\"F1 Score:  {f1_score(y_true, y_pred):.3f}\")\n",
    "    print(f\"AUC:       {roc_auc_score(y_true, y_proba):.3f}\")\n",
    "\n",
    "summarize_metrics(\"Untuned Logistic Regression\", y_test, y_pred_untuned, y_proba_untuned)\n",
    "summarize_metrics(\"Tuned Logistic Regression\", y_test, y_pred_tuned, y_proba_tuned)\n",
    "\n",
    "# --- Side-by-side confusion matrix plots ---\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_untuned, ax=ax[0], cmap=\"Blues\")\n",
    "ax[0].set_title(\"Untuned Logistic Regression\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_tuned, ax=ax[1], cmap=\"Greens\")\n",
    "ax[1].set_title(\"Tuned Logistic Regression\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Side-by-side ROC curves ---\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "RocCurveDisplay.from_predictions(y_test, y_proba_untuned, ax=ax, name=\"Untuned\")\n",
    "RocCurveDisplay.from_predictions(y_test, y_proba_tuned, ax=ax, name=\"Tuned\")\n",
    "ax.set_title(\"ROC Curve Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd67859",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve: Tuned vs Untuned\n",
    "\n",
    "The Precision-Recall (PR) curve is useful for evaluating model performance when the dataset is imbalanced.\n",
    "\n",
    "- **Precision**: What proportion of predicted positives are truly positive?\n",
    "- **Recall**: What proportion of actual positives are correctly predicted?\n",
    "\n",
    "We compare PR curves for both the untuned and tuned logistic regression models to assess how well they retrieve the positive class (`Attrition = 1`) across different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21103e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
    "\n",
    "# Calculate precision-recall points\n",
    "prec_untuned, rec_untuned, _ = precision_recall_curve(y_test, y_proba_untuned)\n",
    "prec_tuned, rec_tuned, _ = precision_recall_curve(y_test, y_proba_tuned)\n",
    "\n",
    "# Plot both\n",
    "plt.figure(figsize=(6, 5))\n",
    "PrecisionRecallDisplay(precision=prec_untuned, recall=rec_untuned).plot(ax=plt.gca(), label=\"Untuned\")\n",
    "PrecisionRecallDisplay(precision=prec_tuned, recall=rec_tuned).plot(ax=plt.gca(), label=\"Tuned\")\n",
    "\n",
    "plt.title(\"Precision-Recall Curve Comparison\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47758619",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Threshold tuning (Decision boundary adjustment)\n",
    "\n",
    "By default, classifiers use a threshold of 0.5 to convert probabilities into class labels.\n",
    "\n",
    "However, in imbalanced classification (like attrition), adjusting this threshold can:\n",
    "- Improve **recall** (catch more attrition cases)\n",
    "- Improve **precision** (reduce false positives)\n",
    "- Maximize **F1 score** (balance both)\n",
    "\n",
    "We analyze how precision, recall, and F1 score change across thresholds and select a custom decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b3556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Generate precision, recall, thresholds\n",
    "prec, rec, thresholds = precision_recall_curve(y_test, y_proba_tuned)\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-8)  # avoid divide-by-zero\n",
    "\n",
    "# Find best threshold by max F1\n",
    "best_idx = np.argmax(f1s)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_f1 = f1s[best_idx]\n",
    "\n",
    "print(f\"ðŸ“Œ Best threshold (by F1): {best_threshold:.3f}\")\n",
    "print(f\"Best F1 score: {best_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(thresholds, prec[:-1], label='Precision', linestyle='--')\n",
    "plt.plot(thresholds, rec[:-1], label='Recall', linestyle='--')\n",
    "plt.plot(thresholds, f1s[:-1], label='F1 Score', linewidth=2)\n",
    "\n",
    "plt.axvline(best_threshold, color='red', linestyle=':')\n",
    "plt.title(\"Precision, Recall, and F1 vs Decision Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f325da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply new threshold to probabilities\n",
    "y_pred_custom = (y_proba_tuned >= best_threshold).astype(int)\n",
    "\n",
    "# Metrics at custom threshold\n",
    "print(\"ðŸ“Š Classification Report (Custom Threshold):\")\n",
    "print(classification_report(y_test, y_pred_custom, digits=3))\n",
    "\n",
    "# Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_custom, cmap='Purples')\n",
    "plt.title(f\"Confusion Matrix (Threshold = {best_threshold:.2f})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f5f03",
   "metadata": {},
   "source": [
    "## 6. Final model selection, evaluation, and export\n",
    "\n",
    "We finalize the best-performing model (tuned Logistic Regression), evaluate it on the test set using our custom threshold, and export it for use in the explainability notebook.\n",
    "\n",
    "Steps:\n",
    "- Retrain tuned model on full SMOTE-resampled training set\n",
    "- Apply custom decision threshold (from F1 optimization)\n",
    "- Evaluate using classification report, confusion matrix, ROC, and PR curve\n",
    "- Export the trained model and threshold using `joblib`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa6078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay,\n",
    "    precision_recall_curve,\n",
    "    PrecisionRecallDisplay\n",
    ")\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Rebuild final model using best parameters\n",
    "final_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(\n",
    "        penalty=grid.best_params_['logreg__penalty'],\n",
    "        C=grid.best_params_['logreg__C'],\n",
    "        solver=grid.best_params_['logreg__solver'],\n",
    "        class_weight=grid.best_params_['logreg__class_weight'],\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Retrain on full resampled training set\n",
    "final_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on test set\n",
    "y_proba_final = final_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_final = (y_proba_final >= best_threshold).astype(int)\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"ðŸ“Š Final Model (Custom Threshold) Evaluation:\")\n",
    "print(classification_report(y_test, y_pred_final, digits=3))\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_proba_final):.3f}\")\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_final, cmap=\"PuBu\")\n",
    "plt.title(f\"Confusion Matrix (Final Model, Threshold = {best_threshold:.2f})\")\n",
    "plt.show()\n",
    "\n",
    "# --- ROC Curve ---\n",
    "RocCurveDisplay.from_predictions(y_test, y_proba_final)\n",
    "plt.title(\"ROC Curve â€“ Final Logistic Regression\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Precision-Recall Curve ---\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba_final)\n",
    "PrecisionRecallDisplay(precision=prec, recall=rec).plot()\n",
    "plt.title(\"Precision-Recall Curve â€“ Final Logistic Regression\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Export model and threshold ---\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "joblib.dump(final_model, \"../models/logreg_final_model.joblib\")\n",
    "joblib.dump(best_threshold, \"../models/logreg_threshold.joblib\")\n",
    "\n",
    "print(\"Final model and threshold exported to '../models/'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm-attrition-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
