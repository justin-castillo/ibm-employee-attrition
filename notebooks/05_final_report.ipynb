{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "665d859e",
   "metadata": {},
   "source": [
    "# Final Report – Predicting Employee Attrition with Explainable ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea64eac",
   "metadata": {},
   "source": [
    "## 1. Project Overview\n",
    "\n",
    "This report presents the final results and business interpretation of a predictive modeling project to identify employees at risk of leaving the company. The objective was to create an accurate and explainable classification model using the IBM HR Analytics dataset.\n",
    "\n",
    "Key techniques:\n",
    "- Custom feature engineering\n",
    "- Cross-validated model selection\n",
    "- Threshold optimization\n",
    "- SHAP and LIME explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0244ce",
   "metadata": {},
   "source": [
    "## 2. Summary of Modeling Pipeline\n",
    "\n",
    "The model used in this report is the best-performing classifier identified during the `03_modeling.ipynb` phase. The pipeline includes:\n",
    "- Preprocessing (encoding, scaling, feature engineering)\n",
    "- Classification using [INSERT MODEL NAME, e.g., `CatBoostClassifier`]\n",
    "- Threshold tuning based on validation AUC and F1 score\n",
    "\n",
    "The model and the optimal threshold were saved and loaded here for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a6b83e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns are missing: {'TrainingTimesLastYear', 'JobRole', 'YearsWithCurrManager', 'Education', 'MonthlyRate', 'Age', 'StockOptionLevel', 'YearsSinceLastPromotion', 'NumCompaniesWorked', 'TotalWorkingYears', 'PerformanceRating', 'Department', 'JobSatisfaction', 'MonthlyIncome', 'DailyRate', 'HourlyRate', 'MaritalStatus', 'Gender', 'JobLevel', 'WorkLifeBalance', 'DistanceFromHome', 'BusinessTravel', 'EducationField', 'YearsInCurrentRole', 'YearsAtCompany', 'OverTime', 'RelationshipSatisfaction', 'EnvironmentSatisfaction', 'JobInvolvement', 'PercentSalaryHike'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m y_test = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m../data/processed/y_test.csv\u001b[39m\u001b[33m\"\u001b[39m).squeeze()\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Predict probabilities and apply custom threshold\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m y_probs = model.predict_proba(X_test)[:, \u001b[32m1\u001b[39m]\n\u001b[32m     33\u001b[39m y_pred = (y_probs >= best_threshold).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Report and print classification metrics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\justi\\Anaconda3\\envs\\ibm-attrition-env\\Lib\\site-packages\\imblearn\\pipeline.py:852\u001b[39m, in \u001b[36mPipeline.predict_proba\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m    850\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[32m    851\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter(with_final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m852\u001b[39m         Xt = transform.transform(Xt)\n\u001b[32m    853\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m].predict_proba(Xt, **params)\n\u001b[32m    855\u001b[39m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\justi\\Anaconda3\\envs\\ibm-attrition-env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = f(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs)\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\justi\\Anaconda3\\envs\\ibm-attrition-env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1090\u001b[39m, in \u001b[36mColumnTransformer.transform\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m   1088\u001b[39m     diff = all_names - \u001b[38;5;28mset\u001b[39m(column_names)\n\u001b[32m   1089\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1092\u001b[39m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[32m   1093\u001b[39m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[32m   1094\u001b[39m     _check_n_features(\u001b[38;5;28mself\u001b[39m, X, reset=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: columns are missing: {'TrainingTimesLastYear', 'JobRole', 'YearsWithCurrManager', 'Education', 'MonthlyRate', 'Age', 'StockOptionLevel', 'YearsSinceLastPromotion', 'NumCompaniesWorked', 'TotalWorkingYears', 'PerformanceRating', 'Department', 'JobSatisfaction', 'MonthlyIncome', 'DailyRate', 'HourlyRate', 'MaritalStatus', 'Gender', 'JobLevel', 'WorkLifeBalance', 'DistanceFromHome', 'BusinessTravel', 'EducationField', 'YearsInCurrentRole', 'YearsAtCompany', 'OverTime', 'RelationshipSatisfaction', 'EnvironmentSatisfaction', 'JobInvolvement', 'PercentSalaryHike'}"
     ]
    }
   ],
   "source": [
    "# Final Evaluation on Untouched Test Set\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    RocCurveDisplay,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# Load model and best threshold\n",
    "model = joblib.load('../models/final_model.joblib')\n",
    "\n",
    "with open(\"../models/best_threshold.json\", \"r\") as f:\n",
    "    best_threshold = json.load(f)[\"best_threshold\"]\n",
    "\n",
    "# Load untouched test set (transformed version expected by model)\n",
    "X_test = pd.read_csv(\"../data/processed/x_test_transformed.csv\")\n",
    "y_test = pd.read_csv(\"../data/processed/y_test.csv\").squeeze()\n",
    "\n",
    "# Predict probabilities and apply custom threshold\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_probs >= best_threshold).astype(int)\n",
    "\n",
    "# Report and print classification metrics\n",
    "print(f\"AUC:         {roc_auc_score(y_test, y_probs):.3f}\")\n",
    "print(f\"Accuracy:    {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Precision:   {precision_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Recall:      {recall_score(y_test, y_pred):.3f}\")\n",
    "print(f\"F1 Score:    {f1_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Purples')\n",
    "plt.title(f\"Confusion Matrix (Threshold = {best_threshold:.2f})\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d43b2fc",
   "metadata": {},
   "source": [
    "## 3. Final Evaluation on Test Set\n",
    "\n",
    "We report performance metrics on an untouched test set, including:\n",
    "\n",
    "- Confusion Matrix\n",
    "- ROC Curve\n",
    "- AUC, Accuracy, Precision, Recall, F1\n",
    "\n",
    "These results reflect the model’s real-world performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c35e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Evaluation on Untouched Test Set\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    RocCurveDisplay,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# Load model and best threshold\n",
    "model = joblib.load(\"../models/final_model.joblib\")\n",
    "with open(\"../models/best_threshold.json\", \"r\") as f:\n",
    "    best_threshold = json.load(f)[\"best_threshold\"]\n",
    "\n",
    "# Load untouched test data\n",
    "X_test = pd.read_csv(\"../data/processed/x_test_transformed.csv\")\n",
    "y_test = pd.read_csv(\"../data/processed/y_test.csv\").squeeze()\n",
    "\n",
    "# Predict probabilities and apply optimal threshold\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_probs >= best_threshold).astype(int)\n",
    "\n",
    "# Compute and print classification metrics\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_probs):.3f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.3f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Purples\")\n",
    "plt.title(f\"Confusion Matrix (Threshold = {best_threshold:.2f})\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)\n",
    "roc_display.plot()\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab0842",
   "metadata": {},
   "source": [
    "## 4. Interpretability Summary\n",
    "\n",
    "We use SHAP and LIME to explain model predictions and provide decision transparency.\n",
    "\n",
    "- **Global explanations** (SHAP summary plot) identify which features most influence predictions.\n",
    "- **Local explanations** (SHAP force plot, LIME) illustrate why specific employees were flagged as at-risk.\n",
    "\n",
    "This enables HR stakeholders to understand both overall trends and individual risk cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadd51cd",
   "metadata": {},
   "source": [
    "## 5. Business Recommendations\n",
    "\n",
    "Based on the model and explanations:\n",
    "\n",
    "- High attrition risk is strongly associated with:\n",
    "  - Low satisfaction scores\n",
    "  - Overtime status\n",
    "  - Low income and long tenure gap\n",
    "\n",
    "**Recommendations:**\n",
    "- Review workload for employees working overtime with low satisfaction.\n",
    "- Explore retention strategies for employees with stagnant roles or low promotion frequency.\n",
    "- Prioritize early intervention for new hires with high experience but low satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd68651",
   "metadata": {},
   "source": [
    "## 6. Limitations & Future Work\n",
    "\n",
    "- The dataset is historical and static; future iterations could integrate time-series employee records.\n",
    "- Future modeling can explore survival analysis or uplift modeling to evaluate impact of interventions.\n",
    "- Additional features (e.g., performance reviews, team dynamics) may improve prediction and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd921ba",
   "metadata": {},
   "source": [
    "## 7. Appendix\n",
    "\n",
    "- Final model: `best_model.joblib`\n",
    "- Optimal threshold: `best_threshold.pkl`\n",
    "- Code and outputs: See `03_modeling.ipynb` and `04_explainability.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm-attrition-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
