{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdfd6efb",
   "metadata": {},
   "source": [
    "# Model Explainability and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15dbb4a",
   "metadata": {},
   "source": [
    "## 1. Notebook overview\n",
    "\n",
    "This notebook explains the predictions made by our final logistic regression model.\n",
    "\n",
    "We focus on model transparency and interpretability using:\n",
    "- **SHAP** (SHapley Additive exPlanations): Global and local explanations\n",
    "- **LIME** (Local Interpretable Model-agnostic Explanations): Per-instance feature influence\n",
    "\n",
    "Objectives:\n",
    "- Understand which features most influence attrition risk\n",
    "- Generate interpretable explanations for key test cases\n",
    "- Provide both global and individual-level insights to support business decision-making"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f034d2c",
   "metadata": {},
   "source": [
    "## 2. Load final model and test data\n",
    "\n",
    "We load:\n",
    "- The trained pipeline (`logreg_final_model.joblib`)\n",
    "- The optimized threshold (`logreg_threshold.joblib`)\n",
    "- The untouched test set (`X_test.csv`, `y_test.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d95e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import shap\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "model_path = \"../models/logreg_final_model.joblib\"\n",
    "threshold_path = \"../models/logreg_threshold.joblib\"\n",
    "data_path = \"../data/processed\"\n",
    "\n",
    "# Load final model pipeline and threshold\n",
    "model = joblib.load(model_path)\n",
    "threshold = joblib.load(threshold_path)\n",
    "\n",
    "# Load test data\n",
    "X_test = pd.read_csv(os.path.join(data_path, \"X_test.csv\"))\n",
    "y_test = pd.read_csv(os.path.join(data_path, \"y_test.csv\"))\n",
    "\n",
    "print(\"Model, threshold, and test data loaded.\")\n",
    "print(f\"Custom threshold: {threshold:.3f}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Extract logistic regression model from pipeline\n",
    "logreg = model.named_steps[\"logreg\"]\n",
    "scaler = model.named_steps[\"scaler\"]\n",
    "\n",
    "# Scale X_test (SHAP needs raw model input)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25fedb",
   "metadata": {},
   "source": [
    "## 3. Global explanation using SHAP\n",
    "\n",
    "We use SHAP (SHapley Additive exPlanations) to interpret global model behavior.\n",
    "\n",
    "Key visualizations:\n",
    "- **Bar Plot**: Mean absolute SHAP value per feature (global importance)\n",
    "- **Beeswarm Plot**: Shows both magnitude and direction of SHAP values\n",
    "- **Summary Plot**: Combines feature impact and value distributions\n",
    "\n",
    "These help us answer:\n",
    "- Which features most affect attrition predictions?\n",
    "- Do they push predictions up (higher risk) or down (lower risk)?\n",
    "- Are effects monotonic or conditional?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use KernelExplainer (model-agnostic) for logistic regression\n",
    "explainer = shap.Explainer(logreg.predict_proba, X_test_scaled)\n",
    "shap_values = explainer(X_test_scaled)\n",
    "\n",
    "# Plot 1: Global feature importance (bar)\n",
    "shap.plots.bar(shap_values, max_display=15)\n",
    "plt.title(\"SHAP Global Feature Importance\")\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Beeswarm plot\n",
    "shap.plots.beeswarm(shap_values, max_display=15)\n",
    "plt.title(\"SHAP Beeswarm Plot\")\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Summary plot (colored by feature values)\n",
    "shap.summary_plot(shap_values.values, features=X_test, feature_names=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06736ec0",
   "metadata": {},
   "source": [
    "## Identify high-risk predictions\n",
    "\n",
    "We rank test set predictions by predicted attrition probability and match them to their actual outcomes:\n",
    "\n",
    "- True Positive (TP): Model correctly predicted attrition\n",
    "- False Positive (FP): Model wrongly predicted attrition\n",
    "- False Negative (FN): Model missed an attrition case\n",
    "- True Negative (TN): Model correctly predicted retention\n",
    "\n",
    "We'll use these to select interesting examples for SHAP force/waterfall plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606881fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Predict probability and apply threshold\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "# Create result DataFrame\n",
    "results = X_test.copy()\n",
    "results['actual'] = y_test.values\n",
    "results['predicted'] = y_pred\n",
    "results['proba'] = y_proba\n",
    "\n",
    "# Classification category\n",
    "conditions = [\n",
    "    (results['actual'] == 1) & (results['predicted'] == 1),\n",
    "    (results['actual'] == 0) & (results['predicted'] == 1),\n",
    "    (results['actual'] == 1) & (results['predicted'] == 0),\n",
    "    (results['actual'] == 0) & (results['predicted'] == 0),\n",
    "]\n",
    "choices = ['TP', 'FP', 'FN', 'TN']\n",
    "results['category'] = np.select(conditions, choices)\n",
    "\n",
    "# Sort by risk\n",
    "high_risk = results.sort_values(by='proba', ascending=False)\n",
    "\n",
    "# Show top 10 highest risk predictions\n",
    "high_risk[['proba', 'actual', 'predicted', 'category']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2c2c3",
   "metadata": {},
   "source": [
    "## 4. Local explanation (SHAP)\n",
    "\n",
    "We select a few test instances (e.g., one true positive, one false negative) and visualize:\n",
    "- Force plots\n",
    "- Waterfall plots\n",
    "\n",
    "Goal: Understand how the model combines features to arrive at each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one example from each case type for SHAP interpretation\n",
    "\n",
    "# Highest-risk correct prediction (TP)\n",
    "sample_tp = results[results['category'] == 'TP'].sort_values(by='proba', ascending=False).head(1)\n",
    "\n",
    "# Highest-risk incorrect prediction (FP)\n",
    "sample_fp = results[results['category'] == 'FP'].sort_values(by='proba', ascending=False).head(1)\n",
    "\n",
    "# Highest-risk missed attrition (FN)\n",
    "sample_fn = results[results['category'] == 'FN'].sort_values(by='proba', ascending=False).head(1)\n",
    "\n",
    "# Lowest-risk correct non-attrition (TN)\n",
    "sample_tn = results[results['category'] == 'TN'].sort_values(by='proba', ascending=True).head(1)\n",
    "\n",
    "# Combine for inspection\n",
    "samples = pd.concat([sample_tp, sample_fp, sample_fn, sample_tn])\n",
    "samples['id'] = ['TP (Correct Risk)',\n",
    "                 'FP (Overpredicted)',\n",
    "                 'FN (Missed Risk)',\n",
    "                 'TN (Correct Low Risk)']\n",
    "samples.set_index('id', inplace=True)\n",
    "\n",
    "# Show table for reference\n",
    "samples[['proba', 'actual', 'predicted', 'category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b845ea41",
   "metadata": {},
   "source": [
    "### SHAP waterfall plots (4 cases)\n",
    "\n",
    "We visualize SHAP waterfall plots for:\n",
    "- **True Positive** (TP): correctly predicted attrition\n",
    "- **False Positive** (FP): overpredicted attrition\n",
    "- **False Negative** (FN): missed attrition\n",
    "- **True Negative** (TN): correctly predicted no attrition\n",
    "\n",
    "These plots show how each feature influences the model‚Äôs decision relative to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82dd0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate SHAP values for entire test set (if needed)\n",
    "explainer = shap.Explainer(logreg.predict_proba, X_test_scaled)\n",
    "shap_values = explainer(X_test_scaled)\n",
    "\n",
    "# Helper: map sample row to original index\n",
    "sample_indices = samples.index.to_list()\n",
    "raw_indices = samples.index.map(lambda label: samples.loc[label].name).to_list()\n",
    "\n",
    "# Display SHAP waterfall plots for each sample\n",
    "for idx, label in zip(raw_indices, sample_indices):\n",
    "    print(f\"\\nüîç {label}\")\n",
    "    shap.plots.waterfall(shap_values[idx], max_display=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd18ef",
   "metadata": {},
   "source": [
    "## LIME explanations (4 cases)\n",
    "\n",
    "We use LIME (Local Interpretable Model-agnostic Explanations) to explain individual predictions.\n",
    "\n",
    "- LIME fits a local linear model near the selected instance.\n",
    "- It tells us which features were most responsible for the model's decision.\n",
    "- These are especially useful for presenting to non-technical stakeholders.\n",
    "\n",
    "We apply LIME to the same four cases:\n",
    "- True Positive\n",
    "- False Positive\n",
    "- False Negative\n",
    "- True Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a33284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Convert scaled data to numpy for LIME\n",
    "X_test_array = X_test_scaled\n",
    "\n",
    "# Set up LIME explainer\n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    training_data=X_test_array,\n",
    "    feature_names=X_test.columns.tolist(),\n",
    "    class_names=['No Attrition', 'Attrition'],\n",
    "    mode='classification',\n",
    "    discretize_continuous=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb14a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the same 4 sample indices\n",
    "for idx, label in zip(raw_indices, sample_indices):\n",
    "    print(f\"\\nüîç {label}\")\n",
    "    \n",
    "    # Explain instance\n",
    "    explanation = lime_explainer.explain_instance(\n",
    "        X_test_array[idx],\n",
    "        model.predict_proba,\n",
    "        num_features=10,\n",
    "        top_labels=1\n",
    "    )\n",
    "    \n",
    "    # Show in notebook\n",
    "    explanation.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531aff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, label in zip(raw_indices, sample_indices):\n",
    "    explanation = lime_explainer.explain_instance(\n",
    "        X_test_array[idx],\n",
    "        model.predict_proba,\n",
    "        num_features=10,\n",
    "        top_labels=1\n",
    "    )\n",
    "    html_path = f\"lime_explanation_{label.replace(' ', '_').lower()}.html\"\n",
    "    explanation.save_to_file(html_path)\n",
    "    print(f\"Saved: {html_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f2d7e7",
   "metadata": {},
   "source": [
    "## SHAP vs LIME ‚Äì Explanation Comparison\n",
    "\n",
    "We compare SHAP and LIME explanations for four key test cases:\n",
    "\n",
    "| Case                  | SHAP (Top 3 features)                                | LIME (Top 3 features)                                |\n",
    "|-----------------------|------------------------------------------------------|------------------------------------------------------|\n",
    "| **TP ‚Äì Correct Risk** | - OverTime = Yes ‚Üë<br>- Age = low ‚Üë<br>- JobLevel = low ‚Üë | - OverTime = Yes ‚Üë<br>- Age = low ‚Üë<br>- JobLevel = low ‚Üë |\n",
    "| **FP ‚Äì Overpredicted**| - MonthlyIncome = high ‚Üì<br>- EnvironmentSatisfaction = low ‚Üë<br>- DistanceFromHome = high ‚Üë | - MonthlyIncome = high ‚Üì<br>- DistanceFromHome = high ‚Üë<br>- JobInvolvement = low ‚Üë |\n",
    "| **FN ‚Äì Missed Risk**  | - OverTime = No ‚Üì<br>- DistanceFromHome = high ‚Üë<br>- JobSatisfaction = low ‚Üë | - OverTime = No ‚Üì<br>- Age = high ‚Üì<br>- TrainingTimesLastYear = low ‚Üë |\n",
    "| **TN ‚Äì Correct Safe** | - Age = high ‚Üì<br>- OverTime = No ‚Üì<br>- MonthlyIncome = high ‚Üì | - Age = high ‚Üì<br>- OverTime = No ‚Üì<br>- JobLevel = high ‚Üì |\n",
    "\n",
    "‚¨ÜÔ∏è = pushes prediction toward \"Attrition\"  \n",
    "‚¨áÔ∏è = pushes prediction toward \"No Attrition\"\n",
    "\n",
    "---\n",
    "\n",
    "**Observations:**\n",
    "- SHAP and LIME generally agree on direction and importance of core features.\n",
    "- SHAP captures global context better (e.g., rare interactions), while LIME is more interpretable per-row.\n",
    "- LIME can surface subtle edge-case influences that SHAP smooths over.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4554678b",
   "metadata": {},
   "source": [
    "## Final Summary and Takeaways\n",
    "\n",
    "In this notebook, we explored the interpretability of our final Logistic Regression model using two complementary techniques: **SHAP** and **LIME**.\n",
    "\n",
    "### üîç Key Findings\n",
    "\n",
    "- **Global importance**:\n",
    "  - Features like `OverTime`, `JobLevel`, `MonthlyIncome`, and `Age` consistently influenced attrition risk.\n",
    "  - `OverTime = Yes` was the most dominant driver of predicted attrition.\n",
    "  \n",
    "- **Local explanations**:\n",
    "  - For individual employees, both SHAP and LIME identified intuitive patterns that align with real-world HR expectations.\n",
    "  - False negatives (missed attrition) often involved employees with less obvious risk indicators (e.g., no overtime, moderate income).\n",
    "  - False positives (incorrectly flagged) tended to involve high earners with some stress-related factors (e.g., high commute distance).\n",
    "\n",
    "- **SHAP vs LIME**:\n",
    "  - SHAP provided **consistent, mathematically grounded** insights tied to the overall model behavior.\n",
    "  - LIME offered **simplified, locally faithful** explanations ideal for stakeholder presentation.\n",
    "  - Both methods agreed on most key contributors, but offered complementary perspectives.\n",
    "\n",
    "### üì¶ Next Steps\n",
    "\n",
    "- Integrate these interpretability tools into a dashboard or reporting pipeline.\n",
    "- Use SHAP and LIME to support **HR decision-making**, especially for proactive retention strategies.\n",
    "- Explore model retraining on newer employee data to adapt to evolving patterns.\n",
    "\n",
    "This completes the modeling and interpretability pipeline. We now have a robust, explainable model that is both **technically sound** and **business-ready**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm-attrition-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
